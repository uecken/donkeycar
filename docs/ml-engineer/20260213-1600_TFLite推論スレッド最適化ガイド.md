# TensorFlow Lite 推論スレッド最適化ガイド

**作成日**: 2026年2月13日 16:00
**対象環境**: Raspberry Pi 4 (Cortex-A72 4コア) + Donkeycar
**対象フレームワーク**: TensorFlow Lite 2.x

---

## 目次

1. [num_threads未設定時のデフォルト動作](#1-num_threads未設定時のデフォルト動作)
2. [スレッド数増加による速度向上の理由](#2-スレッド数増加による速度向上の理由)
3. [マルチコア割り当ての仕組み](#3-マルチコア割り当ての仕組み)
4. [ベンチマークデータ](#4-ベンチマークデータ)
5. [Donkeycarへの最適化実装](#5-donkeycarへの最適化実装)
6. [推奨設定](#6-推奨設定)
7. [推論速度の測定方法](#7-推論速度の測定方法)

---

## 1. num_threads未設定時のデフォルト動作

### 1.1 仕様上の定義

TensorFlow Liteの`num_threads`パラメータには以下の仕様があります：

| 値 | 動作 |
|----|------|
| **-1（デフォルト）** | 実装依存・プラットフォーム依存で自動決定 |
| **0** | マルチスレッド無効化（1スレッドと同等） |
| **1以上** | 指定したスレッド数を使用 |

```python
# TensorFlow Lite Interpreter の初期化
import tensorflow as tf

# デフォルト（-1）: 実装依存
interpreter = tf.lite.Interpreter(model_path="model.tflite")

# スレッド数を明示的に指定
interpreter = tf.lite.Interpreter(
    model_path="model.tflite",
    num_threads=4  # 4スレッドを使用
)
```

### 1.2 デフォルト値（-1）の実際の挙動

**重要**: デフォルト値 `-1` の動作はTensorFlow Liteのバージョンによって異なることが報告されています。

| TFLiteバージョン | num_threads=-1 の挙動 |
|-----------------|----------------------|
| TF Lite 2.5以前 | 約4スレッド相当の性能 |
| TF Lite 2.7以降 | **1スレッド相当の性能** |

この変更により、**明示的なスレッド数指定が推奨**されます。

### 1.3 現在のDonkeycar実装の問題点

現在のDonkeycar（`donkeycar/parts/interpreter.py`）のTfLiteクラスでは、`num_threads`が設定されていません：

```python
# 現在の実装（interpreter.py Line 266）
self.interpreter = tf.lite.Interpreter(model_path=model_path)
# num_threads が未指定のため、デフォルト値 -1 が使用される
```

これにより、**TF Lite 2.7以降では1スレッドでしか動作しない可能性**があります。

---

## 2. スレッド数増加による速度向上の理由

### 2.1 CNNの計算特性

畳み込みニューラルネットワーク（CNN）は本質的に並列化可能な演算が多く含まれています。

#### 並列化可能な演算

```
┌─────────────────────────────────────────────────────────────────┐
│                    CNNの並列化可能な演算                          │
├─────────────────────────────────────────────────────────────────┤
│  1. 畳み込み層（Convolution）                                    │
│     - 各出力ピクセルは独立に計算可能                              │
│     - フィルタ適用は空間的に並列化可能                            │
│                                                                 │
│  2. 行列乗算（GEMM: General Matrix Multiplication）              │
│     - 全結合層の基本演算                                         │
│     - 行・列方向に並列分割可能                                   │
│                                                                 │
│  3. プーリング層（Pooling）                                      │
│     - 各出力領域は独立に計算可能                                 │
│                                                                 │
│  4. 活性化関数（ReLU, Sigmoid等）                                │
│     - 要素ごとに独立した演算                                     │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 畳み込み演算のim2col変換

畳み込み層は、効率的な計算のためにim2col変換を使用して行列乗算（GEMM）に変換されます。

```
入力画像           im2col変換         行列乗算（GEMM）
┌─────────┐       ┌─────────┐       ┌─────────┐
│ ■ ■ ■ ■ │       │ row1    │       │         │
│ ■ ■ ■ ■ │  -->  │ row2    │  x    │ フィルタ │  =  出力
│ ■ ■ ■ ■ │       │ row3    │       │  行列   │
│ ■ ■ ■ ■ │       │ ...     │       │         │
└─────────┘       └─────────┘       └─────────┘

各行の計算は独立 → マルチスレッドで並列処理可能
```

**CNNの演算の70%以上**が畳み込み層による行列乗算であり、これが並列化の主なターゲットとなります。

### 2.3 XNNPACKの役割

XNNPACKは、TensorFlow Liteのデフォルトのマルチスレッド推論エンジンです。

#### XNNPACKの特徴

| 特徴 | 説明 |
|------|------|
| **動的タスク粒度調整** | スレッド数に応じてタスクを細粒度/粗粒度に調整 |
| **ARM NEON最適化** | 128ビットSIMD命令で4つのfloat32を同時処理 |
| **オペレータ融合** | 連続する演算をまとめて最適化 |
| **big.LITTLE対応** | ARMのコア構成に応じた自動調整 |

```
XNNPACKの並列化戦略:

1スレッド: 粗粒度タスク
  [=============== Task 1 ===============]

4スレッド: 細粒度タスク
  [= Task1 =][= Task2 =][= Task3 =][= Task4 =]
       ↓         ↓         ↓         ↓
     Core0     Core1     Core2     Core3
```

### 2.4 ARM NEONによるSIMD演算

Raspberry Pi 4のCortex-A72は、ARM NEON（128ビットSIMD）をサポートしています。

```
ARM NEON 128ビット レジスタ:
┌────────┬────────┬────────┬────────┐
│ float0 │ float1 │ float2 │ float3 │  ← 4つのfloat32を同時処理
└────────┴────────┴────────┴────────┘
    32bit    32bit    32bit    32bit

畳み込みでの活用:
- 4つの重み × 4つの入力値 を1命令で乗算
- 結果: 最大4倍のスループット向上
```

**NEON使用時の性能改善**: 理論上最大4倍（実測では2-3倍程度）

---

## 3. マルチコア割り当ての仕組み

### 3.1 pthreadpoolによるスレッド管理

XNNPACKは、pthreadpoolライブラリを使用してスレッドを管理します。

```
┌─────────────────────────────────────────────────────────────┐
│                    TensorFlow Lite                          │
│  ┌───────────────────────────────────────────────────────┐  │
│  │                      XNNPACK                          │  │
│  │  ┌─────────────────────────────────────────────────┐  │  │
│  │  │               pthreadpool                       │  │  │
│  │  │                                                 │  │  │
│  │  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌───────┐  │  │  │
│  │  │  │ Thread0 │ │ Thread1 │ │ Thread2 │ │Thread3│  │  │  │
│  │  │  └────┬────┘ └────┬────┘ └────┬────┘ └───┬───┘  │  │  │
│  │  └───────┼───────────┼───────────┼──────────┼──────┘  │  │
│  └──────────┼───────────┼───────────┼──────────┼─────────┘  │
└─────────────┼───────────┼───────────┼──────────┼────────────┘
              ↓           ↓           ↓          ↓
         ┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐
         │ Core 0 │  │ Core 1 │  │ Core 2 │  │ Core 3 │
         │(A72)   │  │(A72)   │  │(A72)   │  │(A72)   │
         └────────┘  └────────┘  └────────┘  └────────┘
                Raspberry Pi 4 CPU (Cortex-A72 x4)
```

### 3.2 OSスケジューラとの関係

TensorFlow Liteは、スレッドのCPUアフィニティ（コア固定）を明示的に設定しません。
そのため、**OSのスケジューラがスレッドをコアに割り当て**ます。

#### Linuxスケジューラの動作

```
┌──────────────────────────────────────────────────────┐
│              Linux CFS (Completely Fair Scheduler)   │
├──────────────────────────────────────────────────────┤
│  1. スレッドの負荷履歴を追跡                          │
│  2. 利用可能なコアにスレッドを分散                    │
│  3. 負荷バランスを動的に調整                          │
│  4. コンテキストスイッチでオーバーヘッド発生          │
└──────────────────────────────────────────────────────┘
```

### 3.3 スレッド数 vs CPUコア数の関係

| スレッド数 | コア数 | 動作 |
|-----------|--------|------|
| スレッド < コア | 一部のコアが遊休 | 並列性が不十分 |
| スレッド = コア | 理想的な状態 | 最大効率 |
| スレッド > コア | スケジューリングオーバーヘッド | 性能低下の可能性 |

**推奨**: Raspberry Pi 4では **num_threads=4**（コア数と同数）

### 3.4 big.LITTLE アーキテクチャでの考慮事項

Raspberry Pi 4は同種コア（Cortex-A72 x4）ですが、一部のARMデバイス（スマートフォン等）ではbig.LITTLEアーキテクチャが使用されています。

```
big.LITTLE の例（スマートフォン）:

┌──────────────────┐  ┌──────────────────┐
│   bigコア (高性能) │  │ LITTLEコア (省電力) │
│   Cortex-A76 x2  │  │  Cortex-A55 x4   │
└──────────────────┘  └──────────────────┘

XNNPACKの対応:
- コア間のスレッド移動を検出
- 適切なマイクロカーネルに自動切り替え
```

---

## 4. ベンチマークデータ

### 4.1 Raspberry Pi 4 での実測データ

以下は、公開されているベンチマークデータです。

#### SSDLite MobileNet v2 (300x300, FP32) - Raspberry Pi 4 64-bit

| スレッド数 | 推論時間 (ms) | 1スレッド比 |
|-----------|--------------|------------|
| 1 | 210.46 | 1.00x |
| 2 | 130.44 | 1.61x |
| 3 | 111.67 | 1.88x |
| 4 | 109.60 | 1.92x |

#### SSDLite MobileNet v3 Small (320x320, FP32) - Raspberry Pi 4 64-bit

| スレッド数 | 推論時間 (ms) | 1スレッド比 |
|-----------|--------------|------------|
| 1 | 70.51 | 1.00x |
| 2 | 51.11 | 1.38x |
| 3 | 44.95 | 1.57x |
| 4 | 43.34 | 1.63x |

#### EfficientDet-lite0 (320x320, FP32) - Raspberry Pi 4 64-bit

| スレッド数 | 推論時間 (ms) | 1スレッド比 |
|-----------|--------------|------------|
| 1 | 354.84 | 1.00x |
| 2 | 232.89 | 1.52x |
| 3 | 201.95 | 1.76x |
| 4 | 193.39 | 1.83x |

#### EfficientDet-lite1 (384x384, FP32) - Raspberry Pi 4 64-bit

| スレッド数 | 推論時間 (ms) | 1スレッド比 |
|-----------|--------------|------------|
| 1 | 691.68 | 1.00x |
| 2 | 424.00 | 1.63x |
| 3 | 362.36 | 1.91x |
| 4 | 348.76 | 1.98x |

### 4.2 性能改善の傾向

```
性能改善グラフ（概念図）:

推論速度倍率
    ^
2.0x│                    ●───●  ← 大きいモデル（EfficientDet-lite1）
    │              ●────●
1.8x│        ●────●
    │    ●                  ●───●  ← 中規模モデル
1.6x│  ●              ●────●
    │              ●────●
1.4x│        ●
    │    ●
1.2x│  ●
    │●
1.0x│─────────────────────────────→ スレッド数
    1     2     3     4
```

**観察結果**:
- **1→2スレッド**: 最も大きな改善（約50-60%向上）
- **2→3スレッド**: 追加で15-25%向上
- **3→4スレッド**: 追加で5-10%向上（収穫逓減）

### 4.3 量子化モデルでの効果

INT8量子化モデルでは、さらに高速化が期待できます。

| モデル | 精度 | 1スレッド | 4スレッド | 改善率 |
|--------|------|----------|----------|--------|
| EfficientDet-lite0 | FP32 | 354.84ms | 193.39ms | 1.83x |
| EfficientDet-lite0 | INT8 | ~200ms* | ~80ms* | ~2.5x |

*推定値（XNNPACKのINT8最適化による追加効果）

---

## 5. Donkeycarへの最適化実装

### 5.1 推奨される修正

`donkeycar/parts/interpreter.py` のTfLiteクラスを以下のように修正することを推奨します：

```python
class TfLite(Interpreter):
    """
    This class wraps around the TensorFlow Lite interpreter.
    """

    def __init__(self, num_threads: int = 4):
        """
        Args:
            num_threads: 推論に使用するスレッド数
                        -1: プラットフォームデフォルト（非推奨）
                         0: シングルスレッド
                        1-N: 指定したスレッド数
                        推奨値: CPUコア数（Raspberry Pi 4 では 4）
        """
        super().__init__()
        self.interpreter = None
        self.runner = None
        self.signatures = None
        self.num_threads = num_threads

    def load(self, model_path):
        assert os.path.splitext(model_path)[1] == '.tflite', \
            'TFlitePilot should load only .tflite files'
        logger.info(f'Loading model {model_path} with {self.num_threads} threads')

        # スレッド数を明示的に指定
        self.interpreter = tf.lite.Interpreter(
            model_path=model_path,
            num_threads=self.num_threads
        )

        self.signatures = self.interpreter.get_signature_list()
        self.runner = self.interpreter.get_signature_runner()
        self.input_keys = self.signatures['serving_default']['inputs']
        self.output_keys = self.signatures['serving_default']['outputs']
```

### 5.2 config.py での設定

mycar/config.py に以下の設定を追加：

```python
# TensorFlow Lite 推論設定
TFLITE_NUM_THREADS = 4  # Raspberry Pi 4 のコア数
```

### 5.3 XNNPACKデリゲートの明示的有効化（オプション）

より詳細な制御が必要な場合：

```python
import tensorflow as tf

# XNNPACKデリゲートを明示的に使用
delegate = tf.lite.experimental.load_delegate(
    'libXNNPACK.so',
    options={'num_threads': 4}
)

interpreter = tf.lite.Interpreter(
    model_path='model.tflite',
    experimental_delegates=[delegate]
)
```

---

## 6. 推奨設定

### 6.1 Raspberry Pi 4 向け推奨設定

| 設定項目 | 推奨値 | 理由 |
|---------|-------|------|
| num_threads | 4 | Cortex-A72 4コアを最大活用 |
| モデル精度 | INT8 | 速度向上 + メモリ削減 |
| XNNPACK | 有効（デフォルト） | 自動NEON最適化 |

### 6.2 使用ケース別推奨

| ユースケース | スレッド数 | 理由 |
|-------------|-----------|------|
| 最大推論速度 | 4 | 全コア使用 |
| 他プロセスと共存 | 2-3 | CPUリソースを残す |
| 省電力モード | 1-2 | 発熱・消費電力抑制 |
| ベンチマーク | 4 + コア固定 | 安定した測定 |

### 6.3 コア固定によるベンチマーク（高度な設定）

安定したベンチマーク測定のため、`taskset`でコアを固定できます：

```bash
# CPUコア0-3に固定してDonkeycar実行
taskset -c 0-3 python manage.py drive --model models/pilot.tflite
```

### 6.4 推論速度の目安

Donkeycarのドライブループは通常20Hz（50ms/フレーム）で動作します。

| モデルサイズ | 4スレッド推論時間 | 20Hz達成 |
|-------------|-----------------|---------|
| 小（MobileNet v3 Small相当） | ~40ms | 達成可能 |
| 中（MobileNet v2相当） | ~100ms | 要最適化 |
| 大（EfficientDet相当） | ~200ms | 困難 |

**推奨**: Donkeycarのデフォルトモデル（linear, categorical）は十分軽量であり、4スレッド設定で20Hz動作が可能です。

---

## 7. 推論速度の測定方法

### 7.1 Donkeycar内蔵の測定機能

Donkeycarには、パーツごとの実行時間やフレームレートを測定するための機能が組み込まれています。

#### 7.1.1 PartProfiler（verbose=True）

`vehicle.py`に実装されている`PartProfiler`は、各パーツの実行時間を詳細に記録します。

```python
# donkeycar/vehicle.py
class PartProfiler:
    def __init__(self):
        self.records = {}

    def report(self):
        # 各パーツの max, min, avg, パーセンタイル（50%, 90%, 99%, 99.9%）を表示
        logger.info("Part Profile Summary: (times in ms)")
```

**出力例**:
```
Part Profile Summary: (times in ms)
+-------------------+-------+------+------+-------+-------+-------+--------+
|       part        |  max  | min  | avg  |  50%  |  90%  |  99%  | 99.9%  |
+-------------------+-------+------+------+-------+-------+-------+--------+
|    PiCamera       | 12.34 | 8.21 | 9.45 |  9.12 |  10.8 | 11.9  |  12.1  |
|    TfLitePilot    | 48.52 | 35.6 | 42.1 | 41.23 | 45.67 | 47.89 |  48.12 |
|    PCA9685        |  2.15 | 0.98 | 1.23 |  1.15 |  1.56 |  1.89 |   2.01 |
+-------------------+-------+------+------+-------+-------+-------+--------+
```

#### 7.1.2 FrequencyLogger（FPS表示）

`fps.py`に実装されている`FrequencyLogger`は、リアルタイムのフレームレートを監視します。

```python
# donkeycar/parts/fps.py
class FrequencyLogger:
    def __init__(self, debug_interval=10):
        # debug_interval秒ごとにFPSをログ出力

    def run(self):
        # 現在のFPSとFPS履歴を返す
        return self.fps, self.fps_list

    def shutdown(self):
        # 終了時にmin/max FPSを表示
        logger.info(f"fps (min/max) = {min(self.fps_list):2d} / {max(self.fps_list):2d}")
```

#### 7.1.3 PerfMonitor（パフォーマンスモニター）

`perfmon.py`に実装されている`PerfMonitor`は、CPU使用率、メモリ使用率、実行周波数を監視します。

```python
# donkeycar/parts/perfmon.py
class PerfMonitor:
    def run_threaded(self):
        # CPU使用率、メモリ使用率、実行周波数を返す
        return self._cpu_percent, self._mem_percent, vehicle_frequency
```

### 7.2 myconfig.pyでの設定

#### 7.2.1 FPS表示の有効化

```python
# ~/mycar/myconfig.py

# FPSカウンター有効化
SHOW_FPS = True
FPS_DEBUG_INTERVAL = 10  # 10秒ごとにFPS情報をシェルに出力
```

#### 7.2.2 パフォーマンスモニターの有効化

```python
# ~/mycar/myconfig.py

# パフォーマンスモニター有効化
HAVE_PERFMON = True
```

#### 7.2.3 ログレベルの設定

```python
# ~/mycar/myconfig.py

# コンソールログ有効化
HAVE_CONSOLE_LOGGING = True
LOGGING_LEVEL = 'INFO'  # DEBUG にするとより詳細な情報を表示
```

### 7.3 コマンドラインでの実行

#### 7.3.1 基本的なプロファイリング実行

`V.start()`に`verbose=True`を渡すことで、200ループごとにプロファイルレポートが出力されます。

**manage.pyを修正する方法**:
```python
# ~/mycar/manage.py の最後の部分を修正
# 変更前
V.start(rate_hz=cfg.DRIVE_LOOP_HZ, max_loop_count=cfg.MAX_LOOPS)

# 変更後（プロファイリング有効）
V.start(rate_hz=cfg.DRIVE_LOOP_HZ, max_loop_count=cfg.MAX_LOOPS, verbose=True)
```

#### 7.3.2 実行コマンド例

```bash
# 通常の実行（FPS/PerfMon設定後）
cd ~/mycar
python manage.py drive --model models/pilot.tflite

# コア固定で安定した測定
taskset -c 0-3 python manage.py drive --model models/pilot.tflite

# ログをファイルに保存
python manage.py drive --model models/pilot.tflite 2>&1 | tee profile_log.txt
```

### 7.4 ベンチマークスクリプト例

#### 7.4.1 推論ベンチマークスクリプト（汎用）

モデルの推論時間を直接測定するスクリプトです。

```python
#!/usr/bin/env python3
"""
TensorFlow Lite 推論ベンチマーク
usage: python benchmark_inference.py --model models/pilot.tflite --threads 4
"""
import argparse
import time
import numpy as np

def benchmark_tflite(model_path, num_threads, num_warmup=10, num_runs=100):
    """TFLite推論のベンチマーク"""
    import tensorflow as tf

    # インタープリタ作成
    interpreter = tf.lite.Interpreter(
        model_path=model_path,
        num_threads=num_threads
    )
    interpreter.allocate_tensors()

    # 入出力情報取得
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # ダミー入力作成
    input_shape = input_details[0]['shape']
    input_dtype = input_details[0]['dtype']
    dummy_input = np.random.rand(*input_shape).astype(input_dtype)

    # ウォームアップ
    print(f"Warming up ({num_warmup} runs)...")
    for _ in range(num_warmup):
        interpreter.set_tensor(input_details[0]['index'], dummy_input)
        interpreter.invoke()

    # ベンチマーク実行
    print(f"Benchmarking ({num_runs} runs)...")
    times = []
    for _ in range(num_runs):
        start = time.perf_counter()
        interpreter.set_tensor(input_details[0]['index'], dummy_input)
        interpreter.invoke()
        end = time.perf_counter()
        times.append((end - start) * 1000)  # ms

    # 結果表示
    times = np.array(times)
    print(f"\n=== Benchmark Results (num_threads={num_threads}) ===")
    print(f"Model: {model_path}")
    print(f"Input shape: {input_shape}")
    print(f"Average: {np.mean(times):.2f} ms")
    print(f"Min: {np.min(times):.2f} ms")
    print(f"Max: {np.max(times):.2f} ms")
    print(f"Std: {np.std(times):.2f} ms")
    print(f"50th percentile: {np.percentile(times, 50):.2f} ms")
    print(f"90th percentile: {np.percentile(times, 90):.2f} ms")
    print(f"99th percentile: {np.percentile(times, 99):.2f} ms")
    print(f"Theoretical max FPS: {1000/np.mean(times):.1f} Hz")

    return times

def benchmark_all_threads(model_path, max_threads=4, num_runs=100):
    """全スレッド数でベンチマーク"""
    results = {}
    for threads in range(1, max_threads + 1):
        print(f"\n--- Testing with {threads} thread(s) ---")
        times = benchmark_tflite(model_path, threads, num_runs=num_runs)
        results[threads] = np.mean(times)

    # 比較表示
    print("\n=== Thread Comparison ===")
    print("Threads | Avg (ms) | Speedup")
    print("--------|----------|--------")
    baseline = results[1]
    for threads, avg_time in results.items():
        speedup = baseline / avg_time
        print(f"   {threads}    |  {avg_time:6.2f}  |  {speedup:.2f}x")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="TFLite Inference Benchmark")
    parser.add_argument("--model", required=True, help="Path to .tflite model")
    parser.add_argument("--threads", type=int, default=4, help="Number of threads")
    parser.add_argument("--runs", type=int, default=100, help="Number of benchmark runs")
    parser.add_argument("--compare", action="store_true", help="Compare all thread counts")
    args = parser.parse_args()

    if args.compare:
        benchmark_all_threads(args.model, args.threads, args.runs)
    else:
        benchmark_tflite(args.model, args.threads, num_runs=args.runs)
```

#### 7.4.2 TFLite専用ベンチマークツール

TensorFlowには専用のベンチマークツールが含まれています。

```bash
# TFLite Benchmark Tool のインストール（Raspberry Pi）
pip install tflite-runtime

# または TensorFlow Lite benchmark_model をダウンロード
# https://www.tensorflow.org/lite/performance/measurement

# 使用例
./benchmark_model \
    --graph=models/pilot.tflite \
    --num_threads=4 \
    --num_runs=100 \
    --warmup_runs=10 \
    --enable_op_profiling=true
```

### 7.5 実践的な測定手順

#### 7.5.1 ステップバイステップガイド

1. **設定ファイルの準備**

```python
# ~/mycar/myconfig.py に以下を追加
SHOW_FPS = True
FPS_DEBUG_INTERVAL = 10
HAVE_PERFMON = True
HAVE_CONSOLE_LOGGING = True
LOGGING_LEVEL = 'INFO'
```

2. **manage.pyの修正（オプション）**

```python
# ~/mycar/manage.py
# V.start() を以下のように変更
V.start(rate_hz=cfg.DRIVE_LOOP_HZ, max_loop_count=cfg.MAX_LOOPS, verbose=True)
```

3. **ベンチマーク実行**

```bash
cd ~/mycar

# 標準実行
python manage.py drive --model models/pilot.tflite

# CPUコア固定で安定測定
taskset -c 0-3 python manage.py drive --model models/pilot.tflite

# 固定ループ数で実行（MAX_LOOPS設定時）
# myconfig.py に MAX_LOOPS = 1000 を設定
python manage.py drive --model models/pilot.tflite
```

4. **結果の確認**

- 10秒ごとにFPS情報が表示される
- Ctrl+Cで終了時にPartProfilerのレポートが表示される
- `HAVE_PERFMON=True`ならCPU/メモリ使用率も記録される

#### 7.5.2 結果の解釈方法

| 指標 | 目標値（20Hz動作時） | 説明 |
|------|---------------------|------|
| **FPS** | >= 20 | 目標フレームレート |
| **推論時間(avg)** | < 40ms | 50ms（1/20Hz）の80%以内 |
| **推論時間(99%)** | < 45ms | ジッター考慮 |
| **CPU使用率** | < 80% | 余裕を持った運用 |

#### 7.5.3 ジッター（遅延変動）の確認

```
verbose=True 有効時の警告メッセージ:
WARN::Vehicle: jitter violation in vehicle loop with XXXXms

この警告が頻繁に出る場合:
- num_threads を増やす
- モデルを軽量化する
- DRIVE_LOOP_HZ を下げる
```

### 7.6 測定結果のログ保存

```bash
# ログをファイルに保存
python manage.py drive --model models/pilot.tflite 2>&1 | tee benchmark_$(date +%Y%m%d_%H%M%S).log

# 後で分析
grep "Part Profile" benchmark_*.log
grep "fps" benchmark_*.log
grep "jitter" benchmark_*.log
```

---

## 参考資料

- [TensorFlow Lite Interpreter API (Python)](https://www.tensorflow.org/lite/api_docs/python/tf/lite/Interpreter)
- [TensorFlow Lite Interpreter.Options (Java)](https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter.Options)
- [Accelerating TensorFlow Lite with XNNPACK Integration](https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html)
- [Faster Quantized Inference with XNNPACK](https://blog.tensorflow.org/2021/09/faster-quantized-inference-with-xnnpack.html)
- [XNNPACK GitHub Repository](https://github.com/google/XNNPACK)
- [pthreadpool GitHub Repository](https://github.com/Maratyszcza/pthreadpool)
- [TensorFlow Lite Performance Best Practices](https://www.tensorflow.org/lite/performance/best_practices)
- [NobuoTsukamoto/benchmarks - Raspberry Pi TFLite Benchmarks](https://github.com/NobuoTsukamoto/benchmarks)
- [Parallel Multi Channel Convolution using General Matrix Multiplication](https://arxiv.org/abs/1704.04428)

---

## 更新履歴

| 日付 | 内容 |
|------|------|
| 2026-02-13 | 初版作成 |
| 2026-02-13 | 推論速度測定方法を追記 |
