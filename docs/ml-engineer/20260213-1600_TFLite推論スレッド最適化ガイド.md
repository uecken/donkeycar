# TensorFlow Lite 推論スレッド最適化ガイド

**作成日**: 2026年2月13日 16:00
**対象環境**: Raspberry Pi 4 (Cortex-A72 4コア) + Donkeycar
**対象フレームワーク**: TensorFlow Lite 2.x

---

## 目次

1. [num_threads未設定時のデフォルト動作](#1-num_threads未設定時のデフォルト動作)
2. [スレッド数増加による速度向上の理由](#2-スレッド数増加による速度向上の理由)
3. [マルチコア割り当ての仕組み](#3-マルチコア割り当ての仕組み)
4. [ベンチマークデータ](#4-ベンチマークデータ)
5. [Donkeycarへの最適化実装](#5-donkeycarへの最適化実装)
6. [推奨設定](#6-推奨設定)

---

## 1. num_threads未設定時のデフォルト動作

### 1.1 仕様上の定義

TensorFlow Liteの`num_threads`パラメータには以下の仕様があります：

| 値 | 動作 |
|----|------|
| **-1（デフォルト）** | 実装依存・プラットフォーム依存で自動決定 |
| **0** | マルチスレッド無効化（1スレッドと同等） |
| **1以上** | 指定したスレッド数を使用 |

```python
# TensorFlow Lite Interpreter の初期化
import tensorflow as tf

# デフォルト（-1）: 実装依存
interpreter = tf.lite.Interpreter(model_path="model.tflite")

# スレッド数を明示的に指定
interpreter = tf.lite.Interpreter(
    model_path="model.tflite",
    num_threads=4  # 4スレッドを使用
)
```

### 1.2 デフォルト値（-1）の実際の挙動

**重要**: デフォルト値 `-1` の動作はTensorFlow Liteのバージョンによって異なることが報告されています。

| TFLiteバージョン | num_threads=-1 の挙動 |
|-----------------|----------------------|
| TF Lite 2.5以前 | 約4スレッド相当の性能 |
| TF Lite 2.7以降 | **1スレッド相当の性能** |

この変更により、**明示的なスレッド数指定が推奨**されます。

### 1.3 現在のDonkeycar実装の問題点

現在のDonkeycar（`donkeycar/parts/interpreter.py`）のTfLiteクラスでは、`num_threads`が設定されていません：

```python
# 現在の実装（interpreter.py Line 266）
self.interpreter = tf.lite.Interpreter(model_path=model_path)
# num_threads が未指定のため、デフォルト値 -1 が使用される
```

これにより、**TF Lite 2.7以降では1スレッドでしか動作しない可能性**があります。

---

## 2. スレッド数増加による速度向上の理由

### 2.1 CNNの計算特性

畳み込みニューラルネットワーク（CNN）は本質的に並列化可能な演算が多く含まれています。

#### 並列化可能な演算

```
┌─────────────────────────────────────────────────────────────────┐
│                    CNNの並列化可能な演算                          │
├─────────────────────────────────────────────────────────────────┤
│  1. 畳み込み層（Convolution）                                    │
│     - 各出力ピクセルは独立に計算可能                              │
│     - フィルタ適用は空間的に並列化可能                            │
│                                                                 │
│  2. 行列乗算（GEMM: General Matrix Multiplication）              │
│     - 全結合層の基本演算                                         │
│     - 行・列方向に並列分割可能                                   │
│                                                                 │
│  3. プーリング層（Pooling）                                      │
│     - 各出力領域は独立に計算可能                                 │
│                                                                 │
│  4. 活性化関数（ReLU, Sigmoid等）                                │
│     - 要素ごとに独立した演算                                     │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 畳み込み演算のim2col変換

畳み込み層は、効率的な計算のためにim2col変換を使用して行列乗算（GEMM）に変換されます。

```
入力画像           im2col変換         行列乗算（GEMM）
┌─────────┐       ┌─────────┐       ┌─────────┐
│ ■ ■ ■ ■ │       │ row1    │       │         │
│ ■ ■ ■ ■ │  -->  │ row2    │  x    │ フィルタ │  =  出力
│ ■ ■ ■ ■ │       │ row3    │       │  行列   │
│ ■ ■ ■ ■ │       │ ...     │       │         │
└─────────┘       └─────────┘       └─────────┘

各行の計算は独立 → マルチスレッドで並列処理可能
```

**CNNの演算の70%以上**が畳み込み層による行列乗算であり、これが並列化の主なターゲットとなります。

### 2.3 XNNPACKの役割

XNNPACKは、TensorFlow Liteのデフォルトのマルチスレッド推論エンジンです。

#### XNNPACKの特徴

| 特徴 | 説明 |
|------|------|
| **動的タスク粒度調整** | スレッド数に応じてタスクを細粒度/粗粒度に調整 |
| **ARM NEON最適化** | 128ビットSIMD命令で4つのfloat32を同時処理 |
| **オペレータ融合** | 連続する演算をまとめて最適化 |
| **big.LITTLE対応** | ARMのコア構成に応じた自動調整 |

```
XNNPACKの並列化戦略:

1スレッド: 粗粒度タスク
  [=============== Task 1 ===============]

4スレッド: 細粒度タスク
  [= Task1 =][= Task2 =][= Task3 =][= Task4 =]
       ↓         ↓         ↓         ↓
     Core0     Core1     Core2     Core3
```

### 2.4 ARM NEONによるSIMD演算

Raspberry Pi 4のCortex-A72は、ARM NEON（128ビットSIMD）をサポートしています。

```
ARM NEON 128ビット レジスタ:
┌────────┬────────┬────────┬────────┐
│ float0 │ float1 │ float2 │ float3 │  ← 4つのfloat32を同時処理
└────────┴────────┴────────┴────────┘
    32bit    32bit    32bit    32bit

畳み込みでの活用:
- 4つの重み × 4つの入力値 を1命令で乗算
- 結果: 最大4倍のスループット向上
```

**NEON使用時の性能改善**: 理論上最大4倍（実測では2-3倍程度）

---

## 3. マルチコア割り当ての仕組み

### 3.1 pthreadpoolによるスレッド管理

XNNPACKは、pthreadpoolライブラリを使用してスレッドを管理します。

```
┌─────────────────────────────────────────────────────────────┐
│                    TensorFlow Lite                          │
│  ┌───────────────────────────────────────────────────────┐  │
│  │                      XNNPACK                          │  │
│  │  ┌─────────────────────────────────────────────────┐  │  │
│  │  │               pthreadpool                       │  │  │
│  │  │                                                 │  │  │
│  │  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌───────┐  │  │  │
│  │  │  │ Thread0 │ │ Thread1 │ │ Thread2 │ │Thread3│  │  │  │
│  │  │  └────┬────┘ └────┬────┘ └────┬────┘ └───┬───┘  │  │  │
│  │  └───────┼───────────┼───────────┼──────────┼──────┘  │  │
│  └──────────┼───────────┼───────────┼──────────┼─────────┘  │
└─────────────┼───────────┼───────────┼──────────┼────────────┘
              ↓           ↓           ↓          ↓
         ┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐
         │ Core 0 │  │ Core 1 │  │ Core 2 │  │ Core 3 │
         │(A72)   │  │(A72)   │  │(A72)   │  │(A72)   │
         └────────┘  └────────┘  └────────┘  └────────┘
                Raspberry Pi 4 CPU (Cortex-A72 x4)
```

### 3.2 OSスケジューラとの関係

TensorFlow Liteは、スレッドのCPUアフィニティ（コア固定）を明示的に設定しません。
そのため、**OSのスケジューラがスレッドをコアに割り当て**ます。

#### Linuxスケジューラの動作

```
┌──────────────────────────────────────────────────────┐
│              Linux CFS (Completely Fair Scheduler)   │
├──────────────────────────────────────────────────────┤
│  1. スレッドの負荷履歴を追跡                          │
│  2. 利用可能なコアにスレッドを分散                    │
│  3. 負荷バランスを動的に調整                          │
│  4. コンテキストスイッチでオーバーヘッド発生          │
└──────────────────────────────────────────────────────┘
```

### 3.3 スレッド数 vs CPUコア数の関係

| スレッド数 | コア数 | 動作 |
|-----------|--------|------|
| スレッド < コア | 一部のコアが遊休 | 並列性が不十分 |
| スレッド = コア | 理想的な状態 | 最大効率 |
| スレッド > コア | スケジューリングオーバーヘッド | 性能低下の可能性 |

**推奨**: Raspberry Pi 4では **num_threads=4**（コア数と同数）

### 3.4 big.LITTLE アーキテクチャでの考慮事項

Raspberry Pi 4は同種コア（Cortex-A72 x4）ですが、一部のARMデバイス（スマートフォン等）ではbig.LITTLEアーキテクチャが使用されています。

```
big.LITTLE の例（スマートフォン）:

┌──────────────────┐  ┌──────────────────┐
│   bigコア (高性能) │  │ LITTLEコア (省電力) │
│   Cortex-A76 x2  │  │  Cortex-A55 x4   │
└──────────────────┘  └──────────────────┘

XNNPACKの対応:
- コア間のスレッド移動を検出
- 適切なマイクロカーネルに自動切り替え
```

---

## 4. ベンチマークデータ

### 4.1 Raspberry Pi 4 での実測データ

以下は、公開されているベンチマークデータです。

#### SSDLite MobileNet v2 (300x300, FP32) - Raspberry Pi 4 64-bit

| スレッド数 | 推論時間 (ms) | 1スレッド比 |
|-----------|--------------|------------|
| 1 | 210.46 | 1.00x |
| 2 | 130.44 | 1.61x |
| 3 | 111.67 | 1.88x |
| 4 | 109.60 | 1.92x |

#### SSDLite MobileNet v3 Small (320x320, FP32) - Raspberry Pi 4 64-bit

| スレッド数 | 推論時間 (ms) | 1スレッド比 |
|-----------|--------------|------------|
| 1 | 70.51 | 1.00x |
| 2 | 51.11 | 1.38x |
| 3 | 44.95 | 1.57x |
| 4 | 43.34 | 1.63x |

#### EfficientDet-lite0 (320x320, FP32) - Raspberry Pi 4 64-bit

| スレッド数 | 推論時間 (ms) | 1スレッド比 |
|-----------|--------------|------------|
| 1 | 354.84 | 1.00x |
| 2 | 232.89 | 1.52x |
| 3 | 201.95 | 1.76x |
| 4 | 193.39 | 1.83x |

#### EfficientDet-lite1 (384x384, FP32) - Raspberry Pi 4 64-bit

| スレッド数 | 推論時間 (ms) | 1スレッド比 |
|-----------|--------------|------------|
| 1 | 691.68 | 1.00x |
| 2 | 424.00 | 1.63x |
| 3 | 362.36 | 1.91x |
| 4 | 348.76 | 1.98x |

### 4.2 性能改善の傾向

```
性能改善グラフ（概念図）:

推論速度倍率
    ^
2.0x│                    ●───●  ← 大きいモデル（EfficientDet-lite1）
    │              ●────●
1.8x│        ●────●
    │    ●                  ●───●  ← 中規模モデル
1.6x│  ●              ●────●
    │              ●────●
1.4x│        ●
    │    ●
1.2x│  ●
    │●
1.0x│─────────────────────────────→ スレッド数
    1     2     3     4
```

**観察結果**:
- **1→2スレッド**: 最も大きな改善（約50-60%向上）
- **2→3スレッド**: 追加で15-25%向上
- **3→4スレッド**: 追加で5-10%向上（収穫逓減）

### 4.3 量子化モデルでの効果

INT8量子化モデルでは、さらに高速化が期待できます。

| モデル | 精度 | 1スレッド | 4スレッド | 改善率 |
|--------|------|----------|----------|--------|
| EfficientDet-lite0 | FP32 | 354.84ms | 193.39ms | 1.83x |
| EfficientDet-lite0 | INT8 | ~200ms* | ~80ms* | ~2.5x |

*推定値（XNNPACKのINT8最適化による追加効果）

---

## 5. Donkeycarへの最適化実装

### 5.1 推奨される修正

`donkeycar/parts/interpreter.py` のTfLiteクラスを以下のように修正することを推奨します：

```python
class TfLite(Interpreter):
    """
    This class wraps around the TensorFlow Lite interpreter.
    """

    def __init__(self, num_threads: int = 4):
        """
        Args:
            num_threads: 推論に使用するスレッド数
                        -1: プラットフォームデフォルト（非推奨）
                         0: シングルスレッド
                        1-N: 指定したスレッド数
                        推奨値: CPUコア数（Raspberry Pi 4 では 4）
        """
        super().__init__()
        self.interpreter = None
        self.runner = None
        self.signatures = None
        self.num_threads = num_threads

    def load(self, model_path):
        assert os.path.splitext(model_path)[1] == '.tflite', \
            'TFlitePilot should load only .tflite files'
        logger.info(f'Loading model {model_path} with {self.num_threads} threads')

        # スレッド数を明示的に指定
        self.interpreter = tf.lite.Interpreter(
            model_path=model_path,
            num_threads=self.num_threads
        )

        self.signatures = self.interpreter.get_signature_list()
        self.runner = self.interpreter.get_signature_runner()
        self.input_keys = self.signatures['serving_default']['inputs']
        self.output_keys = self.signatures['serving_default']['outputs']
```

### 5.2 config.py での設定

mycar/config.py に以下の設定を追加：

```python
# TensorFlow Lite 推論設定
TFLITE_NUM_THREADS = 4  # Raspberry Pi 4 のコア数
```

### 5.3 XNNPACKデリゲートの明示的有効化（オプション）

より詳細な制御が必要な場合：

```python
import tensorflow as tf

# XNNPACKデリゲートを明示的に使用
delegate = tf.lite.experimental.load_delegate(
    'libXNNPACK.so',
    options={'num_threads': 4}
)

interpreter = tf.lite.Interpreter(
    model_path='model.tflite',
    experimental_delegates=[delegate]
)
```

---

## 6. 推奨設定

### 6.1 Raspberry Pi 4 向け推奨設定

| 設定項目 | 推奨値 | 理由 |
|---------|-------|------|
| num_threads | 4 | Cortex-A72 4コアを最大活用 |
| モデル精度 | INT8 | 速度向上 + メモリ削減 |
| XNNPACK | 有効（デフォルト） | 自動NEON最適化 |

### 6.2 使用ケース別推奨

| ユースケース | スレッド数 | 理由 |
|-------------|-----------|------|
| 最大推論速度 | 4 | 全コア使用 |
| 他プロセスと共存 | 2-3 | CPUリソースを残す |
| 省電力モード | 1-2 | 発熱・消費電力抑制 |
| ベンチマーク | 4 + コア固定 | 安定した測定 |

### 6.3 コア固定によるベンチマーク（高度な設定）

安定したベンチマーク測定のため、`taskset`でコアを固定できます：

```bash
# CPUコア0-3に固定してDonkeycar実行
taskset -c 0-3 python manage.py drive --model models/pilot.tflite
```

### 6.4 推論速度の目安

Donkeycarのドライブループは通常20Hz（50ms/フレーム）で動作します。

| モデルサイズ | 4スレッド推論時間 | 20Hz達成 |
|-------------|-----------------|---------|
| 小（MobileNet v3 Small相当） | ~40ms | 達成可能 |
| 中（MobileNet v2相当） | ~100ms | 要最適化 |
| 大（EfficientDet相当） | ~200ms | 困難 |

**推奨**: Donkeycarのデフォルトモデル（linear, categorical）は十分軽量であり、4スレッド設定で20Hz動作が可能です。

---

## 参考資料

- [TensorFlow Lite Interpreter API (Python)](https://www.tensorflow.org/lite/api_docs/python/tf/lite/Interpreter)
- [TensorFlow Lite Interpreter.Options (Java)](https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/Interpreter.Options)
- [Accelerating TensorFlow Lite with XNNPACK Integration](https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html)
- [Faster Quantized Inference with XNNPACK](https://blog.tensorflow.org/2021/09/faster-quantized-inference-with-xnnpack.html)
- [XNNPACK GitHub Repository](https://github.com/google/XNNPACK)
- [pthreadpool GitHub Repository](https://github.com/Maratyszcza/pthreadpool)
- [TensorFlow Lite Performance Best Practices](https://www.tensorflow.org/lite/performance/best_practices)
- [NobuoTsukamoto/benchmarks - Raspberry Pi TFLite Benchmarks](https://github.com/NobuoTsukamoto/benchmarks)
- [Parallel Multi Channel Convolution using General Matrix Multiplication](https://arxiv.org/abs/1704.04428)

---

## 更新履歴

| 日付 | 内容 |
|------|------|
| 2026-02-13 | 初版作成 |
