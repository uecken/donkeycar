# DonkeyCar 学習・推論詳細ガイド

**作成日**: 2026年2月3日 17:00
**担当**: ml-engineer
**目的**: DonkeyCar の学習と推論の仕組みを詳細に解説し、カメラ・Throttle・Steering の関係を明確化する

---

## 1. 概要

### 1.1 DonkeyCar の学習方式: 模倣学習（Imitation Learning）

DonkeyCar は **模倣学習（Imitation Learning）** の一種である **Behavioral Cloning（行動クローニング）** を採用している。

#### 1.1.1 模倣学習とは

人間の「お手本」を見て、同じ行動を再現するように学習する手法。

```
【人間の運転】
カメラ画像 + 人間の操作（ステアリング、スロットル）
    ↓ 記録
【学習データ】
    ↓ 教師あり学習
【学習済みモデル】
    ↓ 推論
カメラ画像 → 人間と同じ操作を出力
```

#### 1.1.2 模倣学習の種類

| 手法 | 説明 | Donkey Car |
|------|------|-----------|
| **Behavioral Cloning** | 専門家の行動を直接模倣（教師あり学習） | ✅ 採用 |
| Inverse RL | 専門家の行動から報酬関数を推定 | ❌ |
| GAIL | 敵対的学習で専門家を模倣 | ❌ |
| DAgger | オンラインで修正データを収集 | ❌ |

#### 1.1.3 Behavioral Cloning の特徴

| 項目 | 内容 |
|------|------|
| **利点** | シンプル、データがあればすぐ学習可能、実車で安全 |
| **欠点** | 専門家の質に依存、分布シフト問題（未知の状況に弱い） |
| **Donkey Car** | 人間の運転が上手なほど、良いモデルができる |

#### 1.1.4 強化学習との違い

| 項目 | 模倣学習（Donkey Car） | 強化学習 |
|------|----------------------|---------|
| 学習信号 | 人間の操作（正解ラベル） | 報酬関数 |
| データ収集 | 人間が運転して収集 | エージェントが試行錯誤 |
| 失敗の扱い | 人間が失敗しない限り安全 | 学習中に多数の失敗が発生 |
| 実車適用 | すぐに使える | 危険（シミュレータ推奨） |
| 最適性 | 人間の性能が上限 | 人間を超える可能性あり |

---

### 1.2 End-to-End 学習

DonkeyCar は **End-to-End（E2E）学習** を採用している。
センサー入力から制御出力まで、中間表現なしで直接学習する。

```
┌─────────────────────────────────────────────────────────────┐
│                     学習フェーズ                              │
│  ┌─────────┐    ┌──────────────┐    ┌─────────────────────┐ │
│  │ カメラ画像 │ →  │  CNNモデル   │  → │ Steering + Throttle │ │
│  │ (入力X)   │    │              │    │ (出力Y = 正解ラベル) │ │
│  └─────────┘    └──────────────┘    └─────────────────────┘ │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                     推論フェーズ                              │
│  ┌─────────┐    ┌──────────────┐    ┌─────────────────────┐ │
│  │ カメラ画像 │ →  │  学習済みCNN  │ → │ Steering + Throttle │ │
│  │ (入力X)   │    │              │    │ (予測値)             │ │
│  └─────────┘    └──────────────┘    └─────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

#### 1.2.1 End-to-End vs 従来手法

| 項目 | End-to-End（Donkey Car） | 従来の自動運転 |
|------|-------------------------|---------------|
| パイプライン | 画像 → 制御（直接） | 画像 → 認識 → 計画 → 制御 |
| 中間表現 | なし | レーン検出、物体検出など |
| 設計 | データドリブン | ルールベース |
| 汎化性 | 学習データに依存 | 明示的なルールで対応 |

---

### 1.3 重要な回答

**Q: カメラ映像、Throttle、Steering の3つを組み合わせているか？**

**A: 学習時と推論時で役割が異なる**

| フェーズ | カメラ画像 | Throttle | Steering |
|---------|-----------|----------|----------|
| **学習時** | 入力 (X) | 出力 (Y) ※正解ラベル | 出力 (Y) ※正解ラベル |
| **推論時** | 入力 (X) | 出力 (Y) ※予測値 | 出力 (Y) ※予測値 |

- **入力**: カメラ画像のみ（標準モデル）
- **出力**: Steering（操舵角）と Throttle（スロットル）の2値

### 1.4 データ品質の重要性

模倣学習では **「人間の運転データの品質 = モデルの品質」** となる。

| データの問題 | モデルへの影響 |
|-------------|---------------|
| スロットル反転バグ | モデルが逆の動作を学習 |
| 不安定な運転 | モデルも不安定に |
| 偏ったコース | 特定の状況でしか動作しない |
| ノイズの多いデータ | 予測精度の低下 |

**教訓**: JOYSTICK_THROTTLE_DIR 反転バグのように、データ収集時の設定ミスは致命的。

---

## 2. データ構造

### 2.1 Tub レコード構造

データ収集時に保存されるレコードの構造（`donkeycar/pipeline/types.py:23-41`）:

```python
TubRecordDict = TypedDict(
    'TubRecordDict',
    {
        '_index': int,                          # レコード番号
        '_session_id': str,                     # セッションID
        'cam/image_array': str,                 # 画像ファイルパス
        'user/angle': float,                    # ユーザー入力：操舵角 (-1.0 ~ +1.0)
        'user/throttle': float,                 # ユーザー入力：スロットル (-1.0 ~ +1.0)
        'user/mode': str,                       # 操作モード
        # 以下はオプション
        'imu/acl_x': Optional[float],           # IMU 加速度X
        'imu/acl_y': Optional[float],           # IMU 加速度Y
        'imu/acl_z': Optional[float],           # IMU 加速度Z
        'imu/gyr_x': Optional[float],           # IMU ジャイロX
        'imu/gyr_y': Optional[float],           # IMU ジャイロY
        'imu/gyr_z': Optional[float],           # IMU ジャイロZ
        'behavior/one_hot_state_array': Optional[List[float]],  # 行動状態
        'localizer/location': Optional[int]     # 位置推定
    }
)
```

### 2.2 実際のデータ例

```json
// data/tub_xxx/manifest.json のレコード例
{
    "_index": 1,
    "_session_id": "26-02-03_14-34-46",
    "cam/image_array": "1_cam_image_array_.jpg",
    "user/angle": 0.15,
    "user/throttle": 0.35,
    "user/mode": "user",
    "_timestamp_ms": 1707012886123
}
```

### 2.3 データディレクトリ構成

```
data/
└── tub_2026-02-03_143446/
    ├── manifest.json           # メタデータ・全レコード
    ├── manifest.catalog        # インデックスカタログ
    └── images/
        ├── 1_cam_image_array_.jpg
        ├── 2_cam_image_array_.jpg
        └── ...
```

---

## 3. モデルアーキテクチャ

### 3.1 利用可能なモデルタイプ

**デフォルト設定**（`donkeycar/templates/cfg_complete.py:370`）:

```python
DEFAULT_MODEL_TYPE = 'linear'
```

| モデルタイプ | 説明 | 入力 | 出力 | デフォルト |
|-------------|------|------|------|-----------|
| **`linear`** | 標準モデル（回帰） | 画像 | angle, throttle (連続値) | ✅ |
| `categorical` | カテゴリカルモデル | 画像 | angle(15ビン), throttle(20ビン) | |
| `inferred` | 推論モデル | 画像 | angle のみ、throttle は計算 | |
| `imu` | IMU統合モデル | 画像 + IMU | angle, throttle | |
| `memory` | メモリ付きモデル | 画像 + 過去の出力 | angle, throttle | |
| `behavior` | 行動モデル | 画像 + 行動状態 | angle, throttle | |
| `rnn` | LSTM時系列モデル | 画像シーケンス | angle, throttle | |
| `3d` | 3D CNN時系列モデル | 画像シーケンス | angle, throttle | |

#### 3.1.1 Linear モデルの特徴

| 項目 | 内容 |
|------|------|
| **出力タイプ** | 連続値（-1.0 〜 +1.0） |
| **Steering** | 連続値で出力 |
| **Throttle** | 連続値で出力 |
| **損失関数** | MSE（平均二乗誤差） |
| **用途** | 標準的な自動運転 |

#### 3.1.2 推論時のモデルタイプ指定

```bash
# TFLite モデルを使う場合は --type を明示する必要がある
python manage.py drive --model models/mypilot.tflite --type tflite_linear

# Keras モデルは --type 省略可能（DEFAULT_MODEL_TYPE が使用される）
python manage.py drive --model models/mypilot.h5
```

| ファイル拡張子 | 推奨 --type |
|---------------|-------------|
| `.h5` | `linear`（省略可） |
| `.tflite` | `tflite_linear` |
| `.savedmodel/` | `linear` |

### 3.2 標準 Linear モデルのアーキテクチャ

```python
# donkeycar/parts/keras.py:843-858

def default_n_linear(num_outputs, input_shape=(120, 160, 3)):
    drop = 0.2
    img_in = Input(shape=input_shape, name='img_in')

    # CNN層（core_cnn_layers）
    x = conv2d(24, 5, 2, 1)(x)   # Conv2D: 24フィルタ, 5x5カーネル, stride 2
    x = Dropout(drop)(x)
    x = conv2d(32, 5, 2, 2)(x)   # Conv2D: 32フィルタ
    x = Dropout(drop)(x)
    x = conv2d(64, 5, 2, 3)(x)   # Conv2D: 64フィルタ
    x = Dropout(drop)(x)
    x = conv2d(64, 3, 1, 4)(x)   # Conv2D: 64フィルタ, 3x3カーネル
    x = Dropout(drop)(x)
    x = conv2d(64, 3, 1, 5)(x)   # Conv2D: 64フィルタ
    x = Dropout(drop)(x)
    x = Flatten(name='flattened')(x)

    # 全結合層
    x = Dense(100, activation='relu', name='dense_1')(x)
    x = Dropout(drop)(x)
    x = Dense(50, activation='relu', name='dense_2')(x)
    x = Dropout(drop)(x)

    # 出力層（2出力）
    outputs = [
        Dense(1, activation='linear', name='n_outputs0')(x),  # Steering
        Dense(1, activation='linear', name='n_outputs1')(x)   # Throttle
    ]

    model = Model(inputs=[img_in], outputs=outputs, name='linear')
    return model
```

### 3.3 モデル構造図

```
入力層
┌──────────────────────────────────────┐
│ Input: img_in (120, 160, 3)          │  ← カメラ画像 (RGB)
└──────────────────────────────────────┘
                ↓
CNN層 (特徴抽出)
┌──────────────────────────────────────┐
│ Conv2D 24 filters, 5x5, stride 2     │
│ Dropout 0.2                          │
├──────────────────────────────────────┤
│ Conv2D 32 filters, 5x5, stride 2     │
│ Dropout 0.2                          │
├──────────────────────────────────────┤
│ Conv2D 64 filters, 5x5, stride 2     │
│ Dropout 0.2                          │
├──────────────────────────────────────┤
│ Conv2D 64 filters, 3x3, stride 1     │
│ Dropout 0.2                          │
├──────────────────────────────────────┤
│ Conv2D 64 filters, 3x3, stride 1     │
│ Dropout 0.2                          │
└──────────────────────────────────────┘
                ↓
┌──────────────────────────────────────┐
│ Flatten                              │
└──────────────────────────────────────┘
                ↓
全結合層
┌──────────────────────────────────────┐
│ Dense 100, ReLU                      │
│ Dropout 0.2                          │
├──────────────────────────────────────┤
│ Dense 50, ReLU                       │
│ Dropout 0.2                          │
└──────────────────────────────────────┘
                ↓
出力層
┌─────────────────┐  ┌─────────────────┐
│ n_outputs0 (1)  │  │ n_outputs1 (1)  │
│ = Steering      │  │ = Throttle      │
│ Linear          │  │ Linear          │
└─────────────────┘  └─────────────────┘
```

---

## 4. 学習パイプライン

### 4.1 データフロー

```python
# donkeycar/pipeline/training.py

TubRecord
    ↓
x_transform (画像読み込み・正規化)
    ↓
y_transform (angle, throttle 抽出)
    ↓
tf.data.Dataset
    ↓
model.fit()
```

### 4.2 x_transform（入力変換）

```python
# donkeycar/parts/keras.py (KerasPilot.x_transform)

def x_transform(self, record: TubRecord, img_processor) -> Dict:
    """
    TubRecord から入力データ（画像）を抽出
    """
    img_arr = record.image(processor=img_processor)
    return {'img_in': img_arr}
```

### 4.3 y_transform（出力変換）

```python
# donkeycar/parts/keras.py:339-344 (KerasLinear.y_transform)

def y_transform(self, record: TubRecord) -> Dict:
    """
    TubRecord から出力データ（正解ラベル）を抽出
    """
    angle: float = record.underlying['user/angle']      # 操舵角
    throttle: float = record.underlying['user/throttle'] # スロットル
    return {'n_outputs0': angle, 'n_outputs1': throttle}
```

### 4.4 学習プロセス

```python
# donkeycar/parts/keras.py:138-180 (KerasPilot.train)

def train(self, model_path, train_data, train_steps, batch_size,
          validation_data, validation_steps, epochs, ...):

    # モデルのコンパイル
    self.compile()  # optimizer='adam', loss='mse' (Linear モデルの場合)

    # 学習実行
    history = model.fit(
        x=train_data,
        steps_per_epoch=train_steps,
        batch_size=batch_size,
        callbacks=callbacks,
        validation_data=validation_data,
        validation_steps=validation_steps,
        epochs=epochs,
        verbose=verbose,
        workers=1,
        use_multiprocessing=False
    )
```

### 4.5 損失関数

| モデルタイプ | 損失関数 | 説明 |
|-------------|---------|------|
| `linear` | MSE (Mean Squared Error) | 連続値の回帰 |
| `categorical` | Categorical Cross-Entropy | 離散ビンの分類 |

---

## 5. 推論パイプライン

### 5.1 推論フロー

```python
# donkeycar/parts/keras.py:95-116 (KerasPilot.run)

def run(self, img_arr: np.ndarray, *other_arr) -> Tuple[float, float]:
    """
    ドライブループから呼び出される推論メソッド

    :param img_arr: カメラ画像 (120, 160, 3) uint8
    :return: (angle, throttle) 予測値
    """
    # 1. 画像の正規化 (0-255 → 0.0-1.0)
    norm_img_arr = normalize_image(img_arr)

    # 2. 入力辞書の作成
    input_dict = {'img_in': norm_img_arr}

    # 3. 推論実行
    return self.inference_from_dict(input_dict)

def inference_from_dict(self, input_dict) -> Tuple[float, float]:
    # インタープリター（Keras/TFLite/TensorRT）で予測
    output = self.interpreter.predict_from_dict(input_dict)
    return self.interpreter_to_output(output)

def interpreter_to_output(self, interpreter_out) -> Tuple[float, float]:
    steering = interpreter_out[0]   # n_outputs0
    throttle = interpreter_out[1]   # n_outputs1
    return steering[0], throttle[0]
```

### 5.2 ドライブループでの呼び出し

```python
# donkeycar/templates/basic.py（概念図）

# モデルパーツの追加
V.add(kl,                           # KerasPilot インスタンス
      inputs=['cam/image_array'],   # カメラ画像を入力
      outputs=['pilot/angle', 'pilot/throttle'],  # 予測値を出力
      run_condition='run_pilot')
```

---

## 6. 画像処理

### 6.1 入力画像仕様

| 項目 | 値 | 備考 |
|------|-----|------|
| サイズ | 160 x 120 | 幅 x 高さ |
| チャンネル | 3 (RGB) | または 1 (グレースケール) |
| データ型 | uint8 (0-255) | 収集時 |
| 正規化後 | float32 (0.0-1.0) | 学習・推論時 |

### 6.2 正規化処理

```python
# donkeycar/utils.py

ONE_BYTE_SCALE = 1.0 / 255.0

def normalize_image(img_arr: np.ndarray) -> np.ndarray:
    """
    画像を 0-255 から 0.0-1.0 に正規化
    """
    return img_arr.astype(np.float32) * ONE_BYTE_SCALE
```

### 6.3 データ拡張（Augmentation）

学習時のみ適用される画像拡張:

| 拡張タイプ | 説明 |
|-----------|------|
| 明るさ変更 | ランダムな明るさ調整 |
| ぼかし | Gaussian blur |
| 色調変更 | 色相・彩度の変更 |
| 影追加 | ランダムな影の追加 |

---

## 7. モデル変換

### 7.1 学習から推論への流れ

```
[GPU環境（学習）]
    ↓ 学習
Keras .h5 モデル
    ↓ 変換
[選択肢]
├── TFLite .tflite (Raspberry Pi 向け、量子化可能)
├── TensorRT (Jetson 向け)
└── SavedModel (汎用)
    ↓ 転送
[Raspberry Pi（推論）]
```

### 7.2 TFLite 変換

#### 7.2.1 標準変換（Float32）

**Donkey Car のデフォルトは Float32 です。**

```python
# donkeycar/pipeline/training.py:173-175
if getattr(cfg, 'CREATE_TF_LITE', True):
    tf_lite_model_path = f'{base_path}.tflite'
    keras_model_to_tflite(model_path, tf_lite_model_path)
    # ↑ data_gen を渡していない → Float32
```

#### 7.2.2 変換コード詳細

```python
# donkeycar/parts/interpreter.py:31-56

def keras_to_tflite(model, out_filename, data_gen=None):
    converter = tf.lite.TFLiteConverter.from_keras_model(model)

    if data_gen is not None:
        # Int8 量子化（Coral TPU 用）
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.representative_dataset = data_gen
        converter.inference_input_type = tf.uint8
        converter.inference_output_type = tf.uint8
        # → Int8 量子化モデル

    # data_gen が None の場合 → Float32（標準）
    tflite_model = converter.convert()
    open(out_filename, "wb").write(tflite_model)
```

#### 7.2.3 Float32 vs Int8

| 項目 | Float32（標準） | Int8（量子化） |
|------|----------------|---------------|
| **デフォルト** | ✅ はい | ❌ |
| **対象ハードウェア** | Raspberry Pi 4 | Coral TPU / Edge TPU |
| **精度** | 高い | やや低下 |
| **速度** | 標準 | 高速（TPU使用時） |
| **変換条件** | `data_gen=None` | `data_gen` を渡す |
| **ファイルサイズ** | ~1MB | ~0.3MB |

**結論**: Raspberry Pi 4 で使う `.tflite` ファイルは **Float32** です。

### 7.3 モデル形式の比較

| 形式 | ファイル | サイズ | 推論速度 (Pi4) | 用途 | デフォルト |
|------|---------|--------|---------------|------|-----------|
| Keras | .h5 | ~5MB | 遅い | 学習・デバッグ | |
| **TFLite (Float32)** | .tflite | ~1MB | 中程度 | **標準推論** | ✅ |
| TFLite (Int8) | .tflite | ~0.3MB | 高速（TPU時） | Coral TPU | |
| TensorRT | .engine | ~1MB | 最速 | Jetson のみ | |

### 7.4 ターゲット環境別の推奨

| 環境 | モデル形式 | 備考 |
|------|-----------|------|
| **Raspberry Pi 4** | TFLite Float32 | 標準、追加HW不要 |
| Raspberry Pi + Coral USB | TFLite Int8 | 高速推論 |
| Jetson Nano | TensorRT | GPU活用 |
| 学習環境（GPU） | Keras .h5 | 学習・評価用 |

---

## 8. 設定パラメータ

### 8.1 モデル関連設定

```python
# config.py / myconfig.py

# モデルタイプ
DEFAULT_MODEL_TYPE = 'linear'

# 画像サイズ
IMAGE_W = 160
IMAGE_H = 120
IMAGE_DEPTH = 3  # RGB

# 学習パラメータ
BATCH_SIZE = 128
TRAIN_TEST_SPLIT = 0.8
EPOCHS = 100

# Categorical モデル用
MODEL_CATEGORICAL_MAX_THROTTLE_RANGE = 0.5
```

### 8.2 推論関連設定

```python
# config.py / myconfig.py

# スロットル調整
AI_THROTTLE_MULT = 1.0  # モデル出力に掛ける係数

# モデルパス
MODEL_PATH = None  # 自動検出
```

---

## 9. よくある問題と解決策

### 9.1 Steering が動かない

**原因**: 学習データの操舵角分布が偏っている

**解決策**:
- 左右均等にコーナーを走行
- データの分布を確認・フィルタリング

### 9.2 Throttle が意図と逆に動く

**原因**: `JOYSTICK_THROTTLE_DIR` の設定ミス

**解決策**:
```python
# myconfig.py
JOYSTICK_THROTTLE_DIR = 1.0  # または -1.0
```

### 9.3 モデルが壁に向かって加速

**原因**: 学習データの Throttle が反転している（セクション9.2参照）

**解決策**:
1. `JOYSTICK_THROTTLE_DIR` を正しく設定
2. 全データを再収集
3. モデルを再学習

### 9.4 推論が遅い

**原因**: Keras モデルを直接使用している

**解決策**:
1. TFLite に変換
2. INT8 量子化を適用（Coral TPU使用時）
3. 画像サイズを縮小 (120x160 → 96x128)

### 9.5 よくある誤解: 学習データ数と推論速度

**誤解**: 「学習データが増えると推論が遅くなる」

**事実**: **学習データ数は推論速度に影響しません。**

| フェーズ | データ数の影響 |
|---------|---------------|
| **学習時** | データ数が増える → 学習時間が増加 |
| **推論時** | データ数に依存しない → 常に同じ速度 |

#### 推論速度を決定する要因

```
推論速度 = f(モデルアーキテクチャ, ハードウェア, 入力サイズ)
```

| 要因 | 影響 | Donkey Car の例 |
|------|------|----------------|
| **モデルのパラメータ数** | 多い → 遅い | linear: ~300K パラメータ |
| **レイヤー数・構造** | 深い → 遅い | 5層 CNN + Dense |
| **入力画像サイズ** | 大きい → 遅い | 120×160×3 |
| **ハードウェア** | CPU/GPU/TPU | Raspberry Pi 4 CPU |
| **量子化** | Float32 > Int8 | Float32（標準） |

#### 具体例

```
学習データ: 1,000枚 で学習したモデル  → 推論速度: 40-80ms
学習データ: 100,000枚 で学習したモデル → 推論速度: 40-80ms（同じ）

※ 同じアーキテクチャなら推論速度は同一
```

#### データ数が影響するもの

| 項目 | データ数少ない | データ数多い |
|------|--------------|-------------|
| **推論速度** | 同じ | 同じ |
| **学習時間** | 短い | 長い |
| **モデル精度** | 低い（過学習しやすい） | 高い（汎化性能向上） |

---

## 10. 学習コマンド例

### 10.1 基本的な学習

```bash
# GPU環境（WSL2など）
cd ~/mycar
python train.py --tub data/tub_xxx --model models/mypilot.h5
```

### 10.2 モデルタイプを指定

```bash
# Linear モデル
python train.py --tub data/tub_xxx --model models/mypilot.h5 --type linear

# Categorical モデル
python train.py --tub data/tub_xxx --model models/mypilot.h5 --type categorical
```

### 10.3 TFLite 変換

```bash
# 学習後に自動変換（configで設定）
# または手動変換
python -c "from donkeycar.parts.interpreter import keras_model_to_tflite; \
           keras_model_to_tflite('models/mypilot.h5', 'models/mypilot.tflite')"
```

### 10.4 推論実行

```bash
# Raspberry Pi
cd ~/mycar
python manage.py drive --model models/mypilot.tflite --type tflite_linear
```

---

## 11. まとめ

### 11.1 学習と推論の関係

| 項目 | 学習時 | 推論時 |
|------|--------|--------|
| **入力** | カメラ画像 | カメラ画像 |
| **出力（正解）** | user/angle, user/throttle | - |
| **出力（予測）** | - | pilot/angle, pilot/throttle |
| **環境** | GPU (WSL2/クラウド) | Raspberry Pi |
| **モデル形式** | Keras .h5 | TFLite .tflite |

### 11.2 データの流れ

```
[データ収集]
カメラ → 画像保存
コントローラー → angle/throttle 保存
    ↓
[学習]
画像 (X) + angle/throttle (Y) → モデル学習
    ↓
[推論]
カメラ → 画像 (X) → モデル → angle/throttle 予測 → アクチュエーター
```

### 11.3 重要なポイント

1. **カメラ画像のみが入力**（標準モデル）
2. **Steering と Throttle は両方とも出力**
3. **学習時の angle/throttle は「正解ラベル」として使用**
4. **推論時はカメラ画像から angle/throttle を予測**

---

## 関連資料

- [donkeycar/parts/keras.py](../../donkeycar/parts/keras.py) - モデル定義
- [donkeycar/pipeline/training.py](../../donkeycar/pipeline/training.py) - 学習パイプライン
- [donkeycar/pipeline/types.py](../../donkeycar/pipeline/types.py) - データ構造
- [donkeycar/utils.py](../../donkeycar/utils.py) - ユーティリティ
- [donkeycar/parts/interpreter.py](../../donkeycar/parts/interpreter.py) - TFLite/TensorRT 変換
- [20260203-1520_モデル推論データ解析レポート.md](20260203-1520_モデル推論データ解析レポート.md) - 推論問題の分析

---

## 更新履歴

| 日付 | 内容 |
|------|------|
| 2026-02-03 17:00 | 初版作成 |
| 2026-02-08 18:00 | 模倣学習（Behavioral Cloning）の説明を追加（セクション1.1） |
| 2026-02-08 19:00 | DEFAULT_MODEL_TYPE = 'linear' の説明を追加（セクション3.1） |
| 2026-02-08 19:00 | TFLite Float32 vs Int8 の説明を追加（セクション7.2） |
| 2026-02-08 20:30 | 学習データ数と推論速度の関係を追加（セクション9.5） |
