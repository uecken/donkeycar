# DonkeyCar 学習・推論詳細ガイド

**作成日**: 2026年2月3日 17:00
**担当**: ml-engineer
**目的**: DonkeyCar の学習と推論の仕組みを詳細に解説し、カメラ・Throttle・Steering の関係を明確化する

---

## 1. 概要

### 1.1 DonkeyCar の End-to-End 学習

DonkeyCar は **End-to-End（E2E）学習** を採用している。

```
┌─────────────────────────────────────────────────────────────┐
│                     学習フェーズ                              │
│  ┌─────────┐    ┌──────────────┐    ┌─────────────────────┐ │
│  │ カメラ画像 │ →  │  CNNモデル   │  → │ Steering + Throttle │ │
│  │ (入力X)   │    │              │    │ (出力Y = 正解ラベル) │ │
│  └─────────┘    └──────────────┘    └─────────────────────┘ │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                     推論フェーズ                              │
│  ┌─────────┐    ┌──────────────┐    ┌─────────────────────┐ │
│  │ カメラ画像 │ →  │  学習済みCNN  │ → │ Steering + Throttle │ │
│  │ (入力X)   │    │              │    │ (予測値)             │ │
│  └─────────┘    └──────────────┘    └─────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 重要な回答

**Q: カメラ映像、Throttle、Steering の3つを組み合わせているか？**

**A: 学習時と推論時で役割が異なる**

| フェーズ | カメラ画像 | Throttle | Steering |
|---------|-----------|----------|----------|
| **学習時** | 入力 (X) | 出力 (Y) ※正解ラベル | 出力 (Y) ※正解ラベル |
| **推論時** | 入力 (X) | 出力 (Y) ※予測値 | 出力 (Y) ※予測値 |

- **入力**: カメラ画像のみ（標準モデル）
- **出力**: Steering（操舵角）と Throttle（スロットル）の2値

---

## 2. データ構造

### 2.1 Tub レコード構造

データ収集時に保存されるレコードの構造（`donkeycar/pipeline/types.py:23-41`）:

```python
TubRecordDict = TypedDict(
    'TubRecordDict',
    {
        '_index': int,                          # レコード番号
        '_session_id': str,                     # セッションID
        'cam/image_array': str,                 # 画像ファイルパス
        'user/angle': float,                    # ユーザー入力：操舵角 (-1.0 ~ +1.0)
        'user/throttle': float,                 # ユーザー入力：スロットル (-1.0 ~ +1.0)
        'user/mode': str,                       # 操作モード
        # 以下はオプション
        'imu/acl_x': Optional[float],           # IMU 加速度X
        'imu/acl_y': Optional[float],           # IMU 加速度Y
        'imu/acl_z': Optional[float],           # IMU 加速度Z
        'imu/gyr_x': Optional[float],           # IMU ジャイロX
        'imu/gyr_y': Optional[float],           # IMU ジャイロY
        'imu/gyr_z': Optional[float],           # IMU ジャイロZ
        'behavior/one_hot_state_array': Optional[List[float]],  # 行動状態
        'localizer/location': Optional[int]     # 位置推定
    }
)
```

### 2.2 実際のデータ例

```json
// data/tub_xxx/manifest.json のレコード例
{
    "_index": 1,
    "_session_id": "26-02-03_14-34-46",
    "cam/image_array": "1_cam_image_array_.jpg",
    "user/angle": 0.15,
    "user/throttle": 0.35,
    "user/mode": "user",
    "_timestamp_ms": 1707012886123
}
```

### 2.3 データディレクトリ構成

```
data/
└── tub_2026-02-03_143446/
    ├── manifest.json           # メタデータ・全レコード
    ├── manifest.catalog        # インデックスカタログ
    └── images/
        ├── 1_cam_image_array_.jpg
        ├── 2_cam_image_array_.jpg
        └── ...
```

---

## 3. モデルアーキテクチャ

### 3.1 利用可能なモデルタイプ

| モデルタイプ | 説明 | 入力 | 出力 |
|-------------|------|------|------|
| `linear` | 標準モデル（回帰） | 画像 | angle, throttle (連続値) |
| `categorical` | カテゴリカルモデル | 画像 | angle(15ビン), throttle(20ビン) |
| `inferred` | 推論モデル | 画像 | angle のみ、throttle は計算 |
| `imu` | IMU統合モデル | 画像 + IMU | angle, throttle |
| `memory` | メモリ付きモデル | 画像 + 過去の出力 | angle, throttle |
| `behavior` | 行動モデル | 画像 + 行動状態 | angle, throttle |
| `rnn` | LSTM時系列モデル | 画像シーケンス | angle, throttle |
| `3d` | 3D CNN時系列モデル | 画像シーケンス | angle, throttle |

### 3.2 標準 Linear モデルのアーキテクチャ

```python
# donkeycar/parts/keras.py:843-858

def default_n_linear(num_outputs, input_shape=(120, 160, 3)):
    drop = 0.2
    img_in = Input(shape=input_shape, name='img_in')

    # CNN層（core_cnn_layers）
    x = conv2d(24, 5, 2, 1)(x)   # Conv2D: 24フィルタ, 5x5カーネル, stride 2
    x = Dropout(drop)(x)
    x = conv2d(32, 5, 2, 2)(x)   # Conv2D: 32フィルタ
    x = Dropout(drop)(x)
    x = conv2d(64, 5, 2, 3)(x)   # Conv2D: 64フィルタ
    x = Dropout(drop)(x)
    x = conv2d(64, 3, 1, 4)(x)   # Conv2D: 64フィルタ, 3x3カーネル
    x = Dropout(drop)(x)
    x = conv2d(64, 3, 1, 5)(x)   # Conv2D: 64フィルタ
    x = Dropout(drop)(x)
    x = Flatten(name='flattened')(x)

    # 全結合層
    x = Dense(100, activation='relu', name='dense_1')(x)
    x = Dropout(drop)(x)
    x = Dense(50, activation='relu', name='dense_2')(x)
    x = Dropout(drop)(x)

    # 出力層（2出力）
    outputs = [
        Dense(1, activation='linear', name='n_outputs0')(x),  # Steering
        Dense(1, activation='linear', name='n_outputs1')(x)   # Throttle
    ]

    model = Model(inputs=[img_in], outputs=outputs, name='linear')
    return model
```

### 3.3 モデル構造図

```
入力層
┌──────────────────────────────────────┐
│ Input: img_in (120, 160, 3)          │  ← カメラ画像 (RGB)
└──────────────────────────────────────┘
                ↓
CNN層 (特徴抽出)
┌──────────────────────────────────────┐
│ Conv2D 24 filters, 5x5, stride 2     │
│ Dropout 0.2                          │
├──────────────────────────────────────┤
│ Conv2D 32 filters, 5x5, stride 2     │
│ Dropout 0.2                          │
├──────────────────────────────────────┤
│ Conv2D 64 filters, 5x5, stride 2     │
│ Dropout 0.2                          │
├──────────────────────────────────────┤
│ Conv2D 64 filters, 3x3, stride 1     │
│ Dropout 0.2                          │
├──────────────────────────────────────┤
│ Conv2D 64 filters, 3x3, stride 1     │
│ Dropout 0.2                          │
└──────────────────────────────────────┘
                ↓
┌──────────────────────────────────────┐
│ Flatten                              │
└──────────────────────────────────────┘
                ↓
全結合層
┌──────────────────────────────────────┐
│ Dense 100, ReLU                      │
│ Dropout 0.2                          │
├──────────────────────────────────────┤
│ Dense 50, ReLU                       │
│ Dropout 0.2                          │
└──────────────────────────────────────┘
                ↓
出力層
┌─────────────────┐  ┌─────────────────┐
│ n_outputs0 (1)  │  │ n_outputs1 (1)  │
│ = Steering      │  │ = Throttle      │
│ Linear          │  │ Linear          │
└─────────────────┘  └─────────────────┘
```

---

## 4. 学習パイプライン

### 4.1 データフロー

```python
# donkeycar/pipeline/training.py

TubRecord
    ↓
x_transform (画像読み込み・正規化)
    ↓
y_transform (angle, throttle 抽出)
    ↓
tf.data.Dataset
    ↓
model.fit()
```

### 4.2 x_transform（入力変換）

```python
# donkeycar/parts/keras.py (KerasPilot.x_transform)

def x_transform(self, record: TubRecord, img_processor) -> Dict:
    """
    TubRecord から入力データ（画像）を抽出
    """
    img_arr = record.image(processor=img_processor)
    return {'img_in': img_arr}
```

### 4.3 y_transform（出力変換）

```python
# donkeycar/parts/keras.py:339-344 (KerasLinear.y_transform)

def y_transform(self, record: TubRecord) -> Dict:
    """
    TubRecord から出力データ（正解ラベル）を抽出
    """
    angle: float = record.underlying['user/angle']      # 操舵角
    throttle: float = record.underlying['user/throttle'] # スロットル
    return {'n_outputs0': angle, 'n_outputs1': throttle}
```

### 4.4 学習プロセス

```python
# donkeycar/parts/keras.py:138-180 (KerasPilot.train)

def train(self, model_path, train_data, train_steps, batch_size,
          validation_data, validation_steps, epochs, ...):

    # モデルのコンパイル
    self.compile()  # optimizer='adam', loss='mse' (Linear モデルの場合)

    # 学習実行
    history = model.fit(
        x=train_data,
        steps_per_epoch=train_steps,
        batch_size=batch_size,
        callbacks=callbacks,
        validation_data=validation_data,
        validation_steps=validation_steps,
        epochs=epochs,
        verbose=verbose,
        workers=1,
        use_multiprocessing=False
    )
```

### 4.5 損失関数

| モデルタイプ | 損失関数 | 説明 |
|-------------|---------|------|
| `linear` | MSE (Mean Squared Error) | 連続値の回帰 |
| `categorical` | Categorical Cross-Entropy | 離散ビンの分類 |

---

## 5. 推論パイプライン

### 5.1 推論フロー

```python
# donkeycar/parts/keras.py:95-116 (KerasPilot.run)

def run(self, img_arr: np.ndarray, *other_arr) -> Tuple[float, float]:
    """
    ドライブループから呼び出される推論メソッド

    :param img_arr: カメラ画像 (120, 160, 3) uint8
    :return: (angle, throttle) 予測値
    """
    # 1. 画像の正規化 (0-255 → 0.0-1.0)
    norm_img_arr = normalize_image(img_arr)

    # 2. 入力辞書の作成
    input_dict = {'img_in': norm_img_arr}

    # 3. 推論実行
    return self.inference_from_dict(input_dict)

def inference_from_dict(self, input_dict) -> Tuple[float, float]:
    # インタープリター（Keras/TFLite/TensorRT）で予測
    output = self.interpreter.predict_from_dict(input_dict)
    return self.interpreter_to_output(output)

def interpreter_to_output(self, interpreter_out) -> Tuple[float, float]:
    steering = interpreter_out[0]   # n_outputs0
    throttle = interpreter_out[1]   # n_outputs1
    return steering[0], throttle[0]
```

### 5.2 ドライブループでの呼び出し

```python
# donkeycar/templates/basic.py（概念図）

# モデルパーツの追加
V.add(kl,                           # KerasPilot インスタンス
      inputs=['cam/image_array'],   # カメラ画像を入力
      outputs=['pilot/angle', 'pilot/throttle'],  # 予測値を出力
      run_condition='run_pilot')
```

---

## 6. 画像処理

### 6.1 入力画像仕様

| 項目 | 値 | 備考 |
|------|-----|------|
| サイズ | 160 x 120 | 幅 x 高さ |
| チャンネル | 3 (RGB) | または 1 (グレースケール) |
| データ型 | uint8 (0-255) | 収集時 |
| 正規化後 | float32 (0.0-1.0) | 学習・推論時 |

### 6.2 正規化処理

```python
# donkeycar/utils.py

ONE_BYTE_SCALE = 1.0 / 255.0

def normalize_image(img_arr: np.ndarray) -> np.ndarray:
    """
    画像を 0-255 から 0.0-1.0 に正規化
    """
    return img_arr.astype(np.float32) * ONE_BYTE_SCALE
```

### 6.3 データ拡張（Augmentation）

学習時のみ適用される画像拡張:

| 拡張タイプ | 説明 |
|-----------|------|
| 明るさ変更 | ランダムな明るさ調整 |
| ぼかし | Gaussian blur |
| 色調変更 | 色相・彩度の変更 |
| 影追加 | ランダムな影の追加 |

---

## 7. モデル変換

### 7.1 学習から推論への流れ

```
[GPU環境（学習）]
    ↓ 学習
Keras .h5 モデル
    ↓ 変換
[選択肢]
├── TFLite .tflite (Raspberry Pi 向け、量子化可能)
├── TensorRT (Jetson 向け)
└── SavedModel (汎用)
    ↓ 転送
[Raspberry Pi（推論）]
```

### 7.2 TFLite 変換

```python
# donkeycar/parts/interpreter.py:24-56

def keras_model_to_tflite(in_filename, out_filename, data_gen=None):
    """
    Keras モデルを TFLite に変換
    """
    model = load_model(in_filename)
    converter = tf.lite.TFLiteConverter.from_keras_model(model)

    # 量子化設定（オプション）
    if data_gen:
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.representative_dataset = data_gen
        converter.target_spec.supported_ops = [
            tf.lite.OpsSet.TFLITE_BUILTINS_INT8
        ]

    tflite_model = converter.convert()
    open(out_filename, "wb").write(tflite_model)
```

### 7.3 モデル形式の比較

| 形式 | ファイル | サイズ | 推論速度 (Pi4) | 用途 |
|------|---------|--------|---------------|------|
| Keras | .h5 | ~5MB | 遅い | 学習・デバッグ |
| TFLite (Float32) | .tflite | ~1MB | 中程度 | 標準推論 |
| TFLite (INT8) | .tflite | ~0.3MB | 高速 | 本番推論 |
| TensorRT | .engine | ~1MB | 最速 | Jetson のみ |

---

## 8. 設定パラメータ

### 8.1 モデル関連設定

```python
# config.py / myconfig.py

# モデルタイプ
DEFAULT_MODEL_TYPE = 'linear'

# 画像サイズ
IMAGE_W = 160
IMAGE_H = 120
IMAGE_DEPTH = 3  # RGB

# 学習パラメータ
BATCH_SIZE = 128
TRAIN_TEST_SPLIT = 0.8
EPOCHS = 100

# Categorical モデル用
MODEL_CATEGORICAL_MAX_THROTTLE_RANGE = 0.5
```

### 8.2 推論関連設定

```python
# config.py / myconfig.py

# スロットル調整
AI_THROTTLE_MULT = 1.0  # モデル出力に掛ける係数

# モデルパス
MODEL_PATH = None  # 自動検出
```

---

## 9. よくある問題と解決策

### 9.1 Steering が動かない

**原因**: 学習データの操舵角分布が偏っている

**解決策**:
- 左右均等にコーナーを走行
- データの分布を確認・フィルタリング

### 9.2 Throttle が意図と逆に動く

**原因**: `JOYSTICK_THROTTLE_DIR` の設定ミス

**解決策**:
```python
# myconfig.py
JOYSTICK_THROTTLE_DIR = 1.0  # または -1.0
```

### 9.3 モデルが壁に向かって加速

**原因**: 学習データの Throttle が反転している（セクション9.2参照）

**解決策**:
1. `JOYSTICK_THROTTLE_DIR` を正しく設定
2. 全データを再収集
3. モデルを再学習

### 9.4 推論が遅い

**原因**: Keras モデルを直接使用している

**解決策**:
1. TFLite に変換
2. INT8 量子化を適用
3. 画像サイズを縮小 (120x160 → 96x128)

---

## 10. 学習コマンド例

### 10.1 基本的な学習

```bash
# GPU環境（WSL2など）
cd ~/mycar
python train.py --tub data/tub_xxx --model models/mypilot.h5
```

### 10.2 モデルタイプを指定

```bash
# Linear モデル
python train.py --tub data/tub_xxx --model models/mypilot.h5 --type linear

# Categorical モデル
python train.py --tub data/tub_xxx --model models/mypilot.h5 --type categorical
```

### 10.3 TFLite 変換

```bash
# 学習後に自動変換（configで設定）
# または手動変換
python -c "from donkeycar.parts.interpreter import keras_model_to_tflite; \
           keras_model_to_tflite('models/mypilot.h5', 'models/mypilot.tflite')"
```

### 10.4 推論実行

```bash
# Raspberry Pi
cd ~/mycar
python manage.py drive --model models/mypilot.tflite --type tflite_linear
```

---

## 11. まとめ

### 11.1 学習と推論の関係

| 項目 | 学習時 | 推論時 |
|------|--------|--------|
| **入力** | カメラ画像 | カメラ画像 |
| **出力（正解）** | user/angle, user/throttle | - |
| **出力（予測）** | - | pilot/angle, pilot/throttle |
| **環境** | GPU (WSL2/クラウド) | Raspberry Pi |
| **モデル形式** | Keras .h5 | TFLite .tflite |

### 11.2 データの流れ

```
[データ収集]
カメラ → 画像保存
コントローラー → angle/throttle 保存
    ↓
[学習]
画像 (X) + angle/throttle (Y) → モデル学習
    ↓
[推論]
カメラ → 画像 (X) → モデル → angle/throttle 予測 → アクチュエーター
```

### 11.3 重要なポイント

1. **カメラ画像のみが入力**（標準モデル）
2. **Steering と Throttle は両方とも出力**
3. **学習時の angle/throttle は「正解ラベル」として使用**
4. **推論時はカメラ画像から angle/throttle を予測**

---

## 関連資料

- [donkeycar/parts/keras.py](../../donkeycar/parts/keras.py) - モデル定義
- [donkeycar/pipeline/training.py](../../donkeycar/pipeline/training.py) - 学習パイプライン
- [donkeycar/pipeline/types.py](../../donkeycar/pipeline/types.py) - データ構造
- [donkeycar/utils.py](../../donkeycar/utils.py) - ユーティリティ
- [20260203-1520_モデル推論データ解析レポート.md](20260203-1520_モデル推論データ解析レポート.md) - 推論問題の分析
