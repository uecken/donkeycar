# SLAM 技術調査レポート

**作成日**: 2026年2月8日 19:30
**担当**: ml-engineer + robotcar-engineer
**目的**: Donkey Car への SLAM 統合可能性とショートカット走行の実現方法を調査

---

## 1. SLAM とは

### 1.1 定義

**SLAM** = **S**imultaneous **L**ocalization **A**nd **M**apping
（同時自己位置推定と地図構築）

```
┌─────────────────────────────────────────────────────────────────┐
│                        SLAM の概念                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   センサー入力           処理              出力                 │
│   ┌──────────┐      ┌──────────┐      ┌──────────────┐         │
│   │ LiDAR    │  →   │  SLAM    │  →   │ 位置 (x,y,θ) │         │
│   │ カメラ   │      │アルゴリズム│      │ マップ       │         │
│   │ IMU      │      │          │      │              │         │
│   └──────────┘      └──────────┘      └──────────────┘         │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 1.2 SLAM の種類

| 種類 | センサー | 特徴 | Pi4 対応 |
|------|---------|------|----------|
| **LiDAR SLAM** | 2D/3D LiDAR | 高精度、環境依存少 | ✅ BreezySLAM |
| **Visual SLAM** | 単眼/ステレオカメラ | 安価、テクスチャ依存 | ⚠️ 重い |
| **VIO** | カメラ + IMU | 高速移動対応 | ⚠️ 重い |
| **Depth SLAM** | 深度カメラ | 屋内向け | ⚠️ RealSense |

### 1.3 Donkey Car の現在の方式との比較

| 項目 | Behavioral Cloning | SLAM + Path Planning |
|------|-------------------|---------------------|
| **学習方式** | 模倣学習（End-to-End） | 自己位置推定 + 経路計画 |
| **入力** | カメラ画像のみ | LiDAR / カメラ + IMU |
| **出力** | Steering, Throttle（直接） | 位置 → 経路 → 制御 |
| **地図** | なし（暗黙的） | 明示的なマップ |
| **ショートカット** | ❌ 不可能 | ✅ 可能 |
| **新規コース対応** | 再学習必要 | マップ再構築のみ |

---

## 2. Donkey Car の既存 SLAM 実装

### 2.1 BreezySLAM

Donkey Car には [BreezySLAM](https://github.com/simondlevy/BreezySLAM) が実装済み。

**ソースコード**: [donkeycar/parts/lidar.py:717-743](../../donkeycar/parts/lidar.py#L717-L743)

```python
class BreezySLAM(object):
    '''https://github.com/simondlevy/BreezySLAM'''

    def __init__(self, MAP_SIZE_PIXELS=500, MAP_SIZE_METERS=10):
        from breezyslam.algorithms import RMHC_SLAM
        from breezyslam.sensors import Laser

        laser_model = Laser(
            scan_size=360,
            scan_rate_hz=10.,
            detection_angle_degrees=360,
            distance_no_detection_mm=12000
        )
        MAP_QUALITY = 5
        self.slam = RMHC_SLAM(laser_model, MAP_SIZE_PIXELS, MAP_SIZE_METERS, MAP_QUALITY)

    def run(self, distances, angles, map_bytes):
        # SLAM 更新
        self.slam.update(distances, scan_angles_degrees=angles)

        # 位置取得
        x, y, theta = self.slam.getpos()

        # マップ更新
        if map_bytes is not None:
            self.slam.getmap(map_bytes)

        return x, y, deg2rad(norm_deg(theta))
```

### 2.2 関連パーツ

| パーツ | ファイル | 機能 |
|--------|---------|------|
| **BreezySLAM** | `lidar.py:717` | SLAM アルゴリズム |
| **BreezyMap** | `lidar.py:746` | マップ格納 |
| **MapToImage** | `lidar.py:760` | マップ画像化 |
| **RPLidar** | `lidar.py:267` | RPLidar ドライバー |
| **RPLidar2** | `lidar.py:49` | 改良版ドライバー |
| **LidarPlot** | `lidar.py:393` | LiDAR 可視化 |

### 2.3 サポート LiDAR

| 製品 | クラス | 備考 |
|------|--------|------|
| **RPLidar A1/A2** | `RPLidar`, `RPLidar2` | 推奨 |
| **YDLidar X4** | `YDLidar` | 安価 |
| **TFMini** | `tfmini.py` | 単方向のみ |

---

## 3. 車両運動学（Kinematics）

### 3.1 Bicycle モデル

**ソースコード**: [donkeycar/parts/kinematics.py:31-100](../../donkeycar/parts/kinematics.py#L31-L100)

```python
class Bicycle:
    """
    Bicycle forward kinematics for a car-like vehicle (Ackerman steering)
    """
    def __init__(self, wheel_base: float):
        self.wheel_base = wheel_base
        self.pose = Pose2D()  # (x, y, angle)

    def run(self, forward_distance, steering_angle, timestamp):
        # オドメトリから位置を計算
        # 前進距離 + ステアリング角 → 位置変化
        return (distance, velocity,
                x, y, angle,           # 位置
                x_vel, y_vel, angle_vel,  # 速度
                timestamp)
```

### 3.2 Pose2D

```python
class Pose2D:
    def __init__(self, x=0.0, y=0.0, angle=0.0):
        self.x = x      # X 座標 (m)
        self.y = y      # Y 座標 (m)
        self.angle = angle  # 向き (rad)
```

---

## 4. ショートカット実現のアーキテクチャ

### 4.1 システム構成

```
┌─────────────────────────────────────────────────────────────────┐
│              SLAM + ショートカット アーキテクチャ                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐  │
│  │ センサー  │ →  │ SLAM     │ →  │ 経路計画  │ →  │ 制御器   │  │
│  └──────────┘    └──────────┘    └──────────┘    └──────────┘  │
│       │              │               │               │         │
│       ↓              ↓               ↓               ↓         │
│   ┌──────────┐  ┌──────────┐   ┌──────────┐   ┌──────────┐    │
│   │ LiDAR    │  │位置(x,y,θ)│   │ Waypoints│   │ Steering │    │
│   │ エンコーダ│  │  マップ   │   │ 最短経路  │   │ Throttle │    │
│   │ IMU      │  │          │   │          │   │          │    │
│   └──────────┘  └──────────┘   └──────────┘   └──────────┘    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 4.2 処理フロー

```
【フェーズ1: マップ構築】
┌────────────────────────────────────────┐
│ 1. 手動走行でコース1周                  │
│ 2. LiDAR + SLAM でマップ作成            │
│ 3. マップをファイルに保存               │
└────────────────────────────────────────┘
          ↓
【フェーズ2: 経路計画】
┌────────────────────────────────────────┐
│ 1. マップを読み込み                     │
│ 2. スタート/ゴール地点を設定            │
│ 3. A* または RRT でショートカット経路計算│
│ 4. Waypoints を生成                    │
└────────────────────────────────────────┘
          ↓
【フェーズ3: 自律走行】
┌────────────────────────────────────────┐
│ 1. SLAM で現在位置を推定                │
│ 2. 次の Waypoint を取得                │
│ 3. Pure Pursuit / PID で制御           │
│ 4. Steering + Throttle を出力          │
└────────────────────────────────────────┘
```

### 4.3 必要なコンポーネント

| コンポーネント | 状態 | 実装場所 |
|---------------|------|----------|
| LiDAR ドライバー | ✅ 実装済 | `parts/lidar.py` |
| SLAM (BreezySLAM) | ✅ 実装済 | `parts/lidar.py` |
| マップ保存/読込 | ✅ 実装済 | `parts/lidar.py` |
| 車両運動学 | ✅ 実装済 | `parts/kinematics.py` |
| **経路計画 (A*)** | ❌ 未実装 | 要開発 |
| **Waypoint 追従** | ❌ 未実装 | 要開発 |
| **Pure Pursuit** | ❌ 未実装 | 要開発 |

---

## 5. 経路計画アルゴリズム

### 5.1 A* アルゴリズム

**用途**: グリッドマップ上での最短経路探索

```python
# 概念コード
def a_star(grid_map, start, goal):
    """
    A* 経路計画

    Args:
        grid_map: 2D グリッドマップ (0=通行可, 1=障害物)
        start: 開始位置 (x, y)
        goal: 目標位置 (x, y)

    Returns:
        path: Waypoints のリスト [(x1,y1), (x2,y2), ...]
    """
    open_set = PriorityQueue()
    open_set.put((0, start))
    came_from = {}
    g_score = {start: 0}

    while not open_set.empty():
        current = open_set.get()[1]

        if current == goal:
            return reconstruct_path(came_from, current)

        for neighbor in get_neighbors(current, grid_map):
            tentative_g = g_score[current] + distance(current, neighbor)

            if neighbor not in g_score or tentative_g < g_score[neighbor]:
                came_from[neighbor] = current
                g_score[neighbor] = tentative_g
                f_score = tentative_g + heuristic(neighbor, goal)
                open_set.put((f_score, neighbor))

    return None  # パスが見つからない
```

### 5.2 Pure Pursuit 追従制御

**用途**: Waypoint 列に沿って滑らかに走行

```python
# 概念コード
class PurePursuit:
    def __init__(self, lookahead_distance=0.5):
        self.lookahead = lookahead_distance

    def run(self, current_pose, waypoints):
        """
        Pure Pursuit 制御

        Args:
            current_pose: 現在位置 (x, y, theta)
            waypoints: 経路 [(x1,y1), (x2,y2), ...]

        Returns:
            steering: ステアリング角 (-1.0 ~ 1.0)
        """
        # Lookahead 点を探す
        target = find_lookahead_point(current_pose, waypoints, self.lookahead)

        # ステアリング角を計算
        alpha = math.atan2(target.y - current_pose.y,
                          target.x - current_pose.x) - current_pose.theta

        # Pure Pursuit の公式
        steering = math.atan2(2 * L * math.sin(alpha), self.lookahead)

        return clamp(steering / max_steering, -1.0, 1.0)
```

---

## 6. ハードウェア要件

### 6.1 推奨構成

| 機器 | 推奨製品 | 価格帯 | 用途 |
|------|---------|--------|------|
| **LiDAR** | RPLidar A1M8 | ~10,000円 | 360° 距離計測 |
| **IMU** | MPU6050 / BNO055 | ~1,000円 | 姿勢補正 |
| **エンコーダー** | ホイールエンコーダー | ~2,000円 | オドメトリ |

### 6.2 センサー配置

```
                    [Pi Camera]
                        ↓
          ┌─────────────────────────┐
          │         [RPLidar]        │ ← 上部に設置
          │            ↓            │
          │    ┌─────────────┐      │
          │    │ Raspberry   │      │
          │    │   Pi 4      │      │
          │    └─────────────┘      │
          │    [MPU6050] [PCA9685]  │
          └─────────────────────────┘
                ↓           ↓
            [サーボ]     [モーター]
              ↓           ↓
          [エンコーダー]
```

### 6.3 接続

| センサー | 接続 | Pi GPIO |
|---------|------|---------|
| RPLidar | USB シリアル | /dev/ttyUSB0 |
| MPU6050 | I2C | SDA/SCL (GPIO 2,3) |
| エンコーダー | GPIO | GPIO 17, 27 など |

---

## 7. Pi4 リソース制約

### 7.1 CPU/メモリ使用量（推定）

| コンポーネント | CPU (%) | メモリ (MB) | 備考 |
|---------------|---------|-------------|------|
| LiDAR 読み取り | 5% | 5 | 7 scans/sec |
| BreezySLAM | 10-15% | 50 | RMHC アルゴリズム |
| 経路計画 (A*) | 5% | 10 | マップサイズ依存 |
| Pure Pursuit | 2% | 5 | 軽量 |
| カメラ + CNN | 40-50% | 200 | 既存処理 |
| **合計** | **~75%** | **~270MB** | 余裕あり |

### 7.2 制御周期

| 処理 | 目標周期 | 実現可能性 |
|------|---------|-----------|
| SLAM 更新 | 10 Hz | ✅ |
| 経路計画 | 1 Hz (または事前計算) | ✅ |
| 制御ループ | 20 Hz | ✅ |

---

## 8. 実装手順

### 8.1 Phase 1: LiDAR 動作確認

```bash
# RPLidar 接続確認
ls /dev/ttyUSB*

# BreezySLAM インストール
pip install breezyslam

# LiDAR テスト
cd ~/mycar
python -c "
from donkeycar.parts.lidar import RPLidar2
lidar = RPLidar2()
print(lidar.run())
lidar.shutdown()
"
```

### 8.2 Phase 2: SLAM マップ構築

```python
# slam_mapping.py
from donkeycar.parts.lidar import RPLidar2, BreezySLAM, BreezyMap
import pickle

# パーツ初期化
lidar = RPLidar2()
slam = BreezySLAM(MAP_SIZE_PIXELS=500, MAP_SIZE_METERS=10)
map_bytes = BreezyMap(MAP_SIZE_PIXELS=500)

# マッピングループ
while True:
    measurements = lidar.run()
    distances = [m[0] for m in measurements]
    angles = [m[1] for m in measurements]

    x, y, theta = slam.run(distances, angles, map_bytes.run())
    print(f"Position: ({x:.2f}, {y:.2f}, {theta:.2f})")

# マップ保存
with open('course_map.pkl', 'wb') as f:
    pickle.dump(map_bytes.mapbytes, f)
```

### 8.3 Phase 3: 経路計画（要開発）

```python
# path_planner.py (概念コード)
import numpy as np
from queue import PriorityQueue

class PathPlanner:
    def __init__(self, map_data, resolution=0.02):
        self.grid = self._create_grid(map_data, resolution)
        self.resolution = resolution

    def plan(self, start, goal):
        """A* 経路計画"""
        # グリッド座標に変換
        start_grid = self._to_grid(start)
        goal_grid = self._to_grid(goal)

        # A* 実行
        path = self._a_star(start_grid, goal_grid)

        # ワールド座標に変換
        return [self._to_world(p) for p in path]
```

### 8.4 Phase 4: Waypoint 追従（要開発）

```python
# waypoint_follower.py (概念コード)
class WaypointFollower:
    def __init__(self, waypoints, lookahead=0.3):
        self.waypoints = waypoints
        self.lookahead = lookahead
        self.current_index = 0

    def run(self, x, y, theta):
        """Pure Pursuit 制御"""
        # 最近傍 Waypoint を探す
        target = self._find_target(x, y)

        # ステアリング計算
        steering = self._pure_pursuit(x, y, theta, target)

        # スロットル（距離に応じて調整）
        throttle = self._compute_throttle(x, y, target)

        return steering, throttle
```

---

## 9. 代替アプローチ

### 9.1 Visual SLAM (ORB-SLAM3)

| 項目 | 内容 |
|------|------|
| **利点** | LiDAR 不要、既存カメラ使用可 |
| **欠点** | 計算負荷高、テクスチャ依存 |
| **Pi4 対応** | ⚠️ 困難（Jetson 推奨） |

### 9.2 強化学習によるショートカット発見

| 項目 | 内容 |
|------|------|
| **利点** | 明示的な地図不要、自己発見 |
| **欠点** | 学習に時間がかかる、シミュレータ必要 |
| **適用** | Donkey Gym で学習 → 実車転移 |

### 9.3 ハイブリッドアプローチ

```
【推奨構成】
├── SLAM: 位置推定のみ使用
├── 経路計画: 事前にショートカット経路を計算
└── 制御: Behavioral Cloning + Pure Pursuit の融合
    - 直線: Pure Pursuit で高速走行
    - コーナー: CNN モデルで安全走行
```

---

## 10. まとめ

### 10.1 実現可能性

| 項目 | 状態 | 備考 |
|------|------|------|
| SLAM (BreezySLAM) | ✅ 実装済 | Pi4 で動作可能 |
| LiDAR サポート | ✅ 実装済 | RPLidar, YDLidar |
| マップ構築 | ✅ 実装済 | BreezyMap |
| 経路計画 | ❌ 未実装 | A* 開発必要 |
| Waypoint 追従 | ❌ 未実装 | Pure Pursuit 開発必要 |

### 10.2 必要な追加開発

| 優先度 | タスク | 工数（推定） |
|--------|--------|-------------|
| 1 | A* 経路計画 | 1-2日 |
| 2 | Pure Pursuit 制御 | 1日 |
| 3 | Donkey Car パイプライン統合 | 1-2日 |
| 4 | テスト・調整 | 2-3日 |

### 10.3 ハードウェア調達

| 機器 | 価格 | 調達先 |
|------|------|--------|
| RPLidar A1M8 | ~10,000円 | Amazon / 秋月電子 |
| ホイールエンコーダー | ~2,000円 | Amazon |
| （オプション）BNO055 | ~3,000円 | 秋月電子 |

---

## 11. 次のステップ

| 順序 | タスク | 担当 |
|------|--------|------|
| 1 | RPLidar A1M8 の調達 | robotcar-engineer |
| 2 | LiDAR 接続・動作確認 | robotcar-engineer |
| 3 | BreezySLAM でマップ作成テスト | robotcar-engineer |
| 4 | A* 経路計画の実装 | ml-engineer / system-architect |
| 5 | Pure Pursuit の実装 | robotcar-engineer |
| 6 | Donkey Car パイプライン統合 | system-architect |
| 7 | ショートカット走行テスト | 全員 |

---

## 12. 関連資料

- [donkeycar/parts/lidar.py](../../donkeycar/parts/lidar.py) - LiDAR / SLAM 実装
- [donkeycar/parts/kinematics.py](../../donkeycar/parts/kinematics.py) - 車両運動学
- [BreezySLAM GitHub](https://github.com/simondlevy/BreezySLAM) - SLAM ライブラリ
- [Pure Pursuit 論文](https://www.ri.cmu.edu/pub_files/pub3/coulter_r_craig_1992_1/coulter_r_craig_1992_1.pdf) - 追従制御

---

## 13. 更新履歴

| 日付 | 内容 |
|------|------|
| 2026-02-08 19:30 | 初版作成（ml-engineer + robotcar-engineer） |
